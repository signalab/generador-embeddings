{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HtQx5zDeIb0"
      },
      "source": [
        "# **Signa_Lab ITESO:** Generador de *Embbeddings*\n",
        "\n",
        "## **Cuaderno 01:** Limpieza y depuración de texto para procesar *embeddings* desde cuerpos de texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyCYY7SLPJtv"
      },
      "source": [
        "Cuaderno de código abierto diseñado para importar cualquier cuerpo de texto separado por filas, en formato CSV o Excel, limpiarlo (*stopwords*, URLs) y depurarlo desde [diccionarios personalizados](https://drive.google.com/file/d/1zK214W0pBRYn9lnY_MDJYhEEc6pI3L6F/view?usp=drive_link) (opcional) para optimizar su posterior procesamiento para generar incrustaciones de texto (*embeddings*) de cada fila, con ayuda de modelos de lenguaje de la librería [sentence-transformers](https://www.sbert.net/), alojados en repositorios de [HuggingFace](https://huggingface.co/sentence-transformers) (en la nube) o descargados localmente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD-wOGZQ5Ki6"
      },
      "source": [
        "## 1. Importar librerías y archivos de datos a depurar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOuliduEPJtw"
      },
      "source": [
        "**Instalar librerías necesarias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kidkEvPHPJtw"
      },
      "outputs": [],
      "source": [
        "# Instalar librerías de Python necesarias\n",
        "\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install difflib\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install scipy\n",
        "!pip install numpy\n",
        "!pip install plotly\n",
        "!pip install time\n",
        "!pip install tqdm\n",
        "!pip install operator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi3kGn_FLC6T"
      },
      "source": [
        "**Importar librerías** necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ3xsq9LK2Sr"
      },
      "outputs": [],
      "source": [
        "# Importar librerías de Python necesarias\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import sys\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from scipy.stats import gaussian_kde\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import math\n",
        "import operator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfsq2zLpPJtx"
      },
      "source": [
        "**Importar archivos de datos** con registros recibidos y diccionario para depuración por descarte:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x30O95HHlHby"
      },
      "outputs": [],
      "source": [
        "# PARA UN SOLO ARCHIVO (individual):\n",
        "# Importar un solo archivo. Especificar ruta de archivo y extensión correspondiente a su formato:\n",
        "ruta = \"./nombre-archivo.csv\" # archivo CSV\n",
        "# ruta = \"./nombre-archivo.xlsx\" # archivo Excel\n",
        "\n",
        "# Crear data frame con datos importados\n",
        "df = pd.read_csv(ruta) # desde archivo CSV\n",
        "# df = pd.read_excel(ruta) # desde archivo Excel\n",
        "\n",
        "\n",
        "# PARA MÚLTIPLES ARCHIVOS (concatenar):\n",
        "# Importar múltiples archivos para concatenar. Especificar rutas y extensiones:\n",
        "# ruta1 = \"./nombre-archivo1.csv\" # importar CSV\n",
        "# ruta2 = \"./nombre-archivo2.csv\" # importar CSV\n",
        "\n",
        "# ruta1 = \"./nombre-archivo1.xlsx\" # importar Excel\n",
        "# ruta2 = \"./nombre-archivo1.xlsx\" # importar Excel\n",
        "\n",
        "# Crear data frames por cada archivo de datos importados:\n",
        "\n",
        "# desde CSV:\n",
        "# df1 = pd.read_csv(ruta1)\n",
        "# df2 = pd.read_csv(ruta2)\n",
        "\n",
        "# desde Excel:\n",
        "# df1 = pd.read_excel(ruta1) # desde archivo Excel\n",
        "# df2 = pd.read_excel(ruta2) # desde archivo Excel\n",
        "\n",
        "\n",
        "# NOMBRE DEL PROYECTO\n",
        "# Especificar nombre del proyecto, que se usará para nombrar los archivos de datos a generar y descargar:\n",
        "nombreProyecto = 'nombre-proyecto'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HptxpF1lHby"
      },
      "source": [
        "Revisar **número de registros importados**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Para un solo archivo importado:\n",
        "\n",
        "# Revisar el número de filas y columnas en archivo:\n",
        "print(\"Filas y columnas en archivo importado:\")\n",
        "df.shape"
      ],
      "metadata": {
        "id": "3uxA0NN0lEbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para múltiples archivos importados:\n",
        "\n",
        "# Revisar el número de filas y columnas en archivo 1\n",
        "# print(\"Filas y columnas en archivo 1:\")\n",
        "# df1.shape"
      ],
      "metadata": {
        "id": "VMEwJDVNpUz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para múltiples archivos importados:\n",
        "\n",
        "# Revisar el número de filas y columnas en archivo 2\n",
        "# print(\"Filas y columnas en archivo 2:\")\n",
        "# df2.shape"
      ],
      "metadata": {
        "id": "benVgCuHpYhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhOkQvg1lHby"
      },
      "outputs": [],
      "source": [
        "# Para múltiples archivos importados:\n",
        "\n",
        "# Concatenar en una tabla todos los archivos importados y revisar el número de filas y columnas totales\n",
        "# df = pd.concat([df1, df2],axis=0,ignore_index=True)\n",
        "# df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jcaDQwpJPJtz"
      },
      "outputs": [],
      "source": [
        "# Crear copia de trabajo de la tabla de registros importados\n",
        "dfTest = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eghyYictPJtz"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla con los registros importados\n",
        "dfTest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nheXTnM7PJt0"
      },
      "outputs": [],
      "source": [
        "# Exportar archivo CSV con tabla de registros importados (y concatenados, en el caso de múltiples archivos)\n",
        "dfTest.to_csv(f\"{nombreProyecto.split('.')[0]}_registros-importados.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hPbcqVvlHbz"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Limpieza y depuración de registros importados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dF8kzOgjyn8C"
      },
      "source": [
        "Generar identificadores únicos (IDs) por registro:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para asignar IDs únicos a cada fila en el data frame indicado como parámetro, comenzando desde '1000001'.\n",
        "def assign_unique_ids(df):\n",
        "    # Inicializar contador para IDs\n",
        "    id_counter = 1000001\n",
        "\n",
        "    # Crear copia de data frame original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Iterar a través de las filas del data frame\n",
        "    for index, _ in enumerate(df_copy.index):\n",
        "        # Dar formato a ID con ceros adicionales e incorporarlo al data frame\n",
        "        formatted_id = str(id_counter).zfill(7)  # Se asegura de que sea un ID de 7 dígitos, agregando ceros cuando sea necesario\n",
        "        df_copy.loc[index, 'id'] = formatted_id\n",
        "\n",
        "        # Incrementar el contador del ID para la siguiente iteración\n",
        "        id_counter += 1\n",
        "\n",
        "    return df_copy"
      ],
      "metadata": {
        "id": "EhuSrvzjr1Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar función para asignar IDs a cada registro\n",
        "if __name__ == \"__main__\":\n",
        "    # Invocar la función con data frame de trabajo\n",
        "    df_ids = assign_unique_ids(dfTest)\n",
        "\n",
        "df_ids"
      ],
      "metadata": {
        "id": "UyPVvBIWsVzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzOHgfPO0bUM"
      },
      "outputs": [],
      "source": [
        "# Sobreescribir data frame con nueva tabla con IDs generados\n",
        "df = df_ids\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13e2f_ioPJt8"
      },
      "outputs": [],
      "source": [
        "# Revisar el número de filas y columnas en tabla de registros con IDs generados\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVfoXAtilHbz"
      },
      "source": [
        "### Limpieza de texto sin aporte semántico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYQEQnPllHb0"
      },
      "outputs": [],
      "source": [
        "# Definir función para limpiar usuarios, hashtags y URLs\n",
        "def limpiar_texto(texto):\n",
        "  # Eliminar usuarios (opcional, comentar siguiente línea para omitirlo)\n",
        "  texto = re.sub(r\"(?<!\\w)@(\\w+)(?!\\w)\", \"\", texto)\n",
        "\n",
        "  # Eliminar hashtags (opcional, comentar siguiente línea para omitirlo)\n",
        "  # texto = re.sub(r\"(?<!\\w)#(\\w+)(?!\\w)\", \"\", texto)\n",
        "\n",
        "  # Eliminar URLs (opcional, comentar siguiente línea para omitirlo)\n",
        "  texto = re.sub(r\"(http|https|ftp)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\", \"\", texto)\n",
        "  texto = texto.lstrip(\". \")\n",
        "\n",
        "  return texto.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thOE2xz6lHb0"
      },
      "source": [
        "**Ejecutar funciones y agregar nueva columna con registros con texto limpio:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_asTAXQzlHb0"
      },
      "outputs": [],
      "source": [
        "# Definir función para agregar nueva columna con texto limpio (clean_text)\n",
        "def agregarCleanTextADf(df, colText):\n",
        "    dfW = df.copy()\n",
        "    dfW[\"clean_text\"] = None\n",
        "\n",
        "    for index, row in dfW.iterrows():\n",
        "        text = str(row[colText])\n",
        "        cleaned_text = limpiar_texto(text)\n",
        "        dfW.at[index, \"clean_text\"] = cleaned_text\n",
        "\n",
        "    return dfW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4cODeeVlHb0"
      },
      "outputs": [],
      "source": [
        "# Especificar nombre de columna con texto para limpiar y procesar embeddings\n",
        "text_column = \"nombre-columna-texto\"\n",
        "\n",
        "# Ejecutar limpieza de usuarios, hashtags y URLs\n",
        "dfCleanText = agregarCleanTextADf(df, text_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VgY4nY_lHb0"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla con nueva columna de datos limpios (clean_text)\n",
        "dfCleanText.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f86YTwWJlHb0"
      },
      "source": [
        "### Eliminar palabras vacías (*stop words*):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMi4M_w-lHb0"
      },
      "source": [
        "Descargar dependencias de librería NLTK para **limpieza de palabras vacías (*stop words*):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz81G4vTlHb0"
      },
      "outputs": [],
      "source": [
        "# Descargar las stop words en español e inglés\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Lista de stopwords en español\n",
        "stopwords_es = nltk.corpus.stopwords.words('spanish')\n",
        "# stopwords_en = nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1BMaiH0lHb1"
      },
      "source": [
        "**Función para eliminar palabras vacías (*stop words*)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8l3-6rvlHb1"
      },
      "outputs": [],
      "source": [
        "# Definir función para eliminar palabras vacías (stopwords) y signos\n",
        "def delete_stopwords(texto):\n",
        "\n",
        "  # Tokenizar el texto\n",
        "  tokens = nltk.word_tokenize(texto)\n",
        "\n",
        "  # Eliminar signos de puntuación\n",
        "  tokens = [token for token in tokens if token.isalnum()]\n",
        "\n",
        "  # Eliminar stop words\n",
        "  tokens = [token for token in tokens if token not in stopwords_es]\n",
        "\n",
        "  # Convertir la lista de tokens a un string\n",
        "  texto_limpio = \" \".join(tokens)\n",
        "\n",
        "  return texto_limpio.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uTm6hRilHb1"
      },
      "source": [
        "Ejecutar funciones y **agregar columna con texto sin *stop words* (sem_text):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWux9PURlHb1"
      },
      "outputs": [],
      "source": [
        "# Definir función para eliminar palabras vacías y agregar nueva columna con el resultado (sem_text)\n",
        "def agregarSemTextADf(df, colText):\n",
        "    dfW = df.copy()\n",
        "    dfW[\"sem_text\"] = None\n",
        "\n",
        "    for index, row in dfW.iterrows():\n",
        "        text = row[colText]\n",
        "        cleaned_text = delete_stopwords(text)\n",
        "        dfW.at[index, \"sem_text\"] = cleaned_text\n",
        "\n",
        "    return dfW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnZelINHlHb1"
      },
      "outputs": [],
      "source": [
        "# Ejecutar función para eliminar palabras vacías y agregar nueva columna con el resultado (sem_text)\n",
        "dfFinal = agregarSemTextADf(dfCleanText, \"clean_text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nZBtGY0lHb1"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de datos con nueva columna de registros con texto sin palabras vacías (sem_text)\n",
        "dfFinal.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GlzbI6mPJt-"
      },
      "outputs": [],
      "source": [
        "# Revisar el número de filas y columnas en tabla de registros con texto sin palabras vacías (sem_text)\n",
        "dfFinal.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPu5tTLxPJt-"
      },
      "outputs": [],
      "source": [
        "# Exportar archivo CSV con tabla completa de registros importados con IDs y texto sin palabras vacías (sem_text)\n",
        "dfFinal.to_csv(f\"{nombreProyecto.split('.')[0]}_registros-semtext.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHNNKJdTlHb1"
      },
      "source": [
        "## 3. Depuración de registros desde diccionarios personalizados con términos de descarte\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t-7ye11YwN2"
      },
      "source": [
        "A partir de diccionarios personalizados con términos de descarte, depurar registros que contengan alguno de ellos.\n",
        "\n",
        "El diccionario debe subirse en formato CSV y contener, al menos, los siguientes campos:\n",
        "\n",
        "| palabra | tipo | categoría | diccionario |\n",
        "|---------|------|-----------|-------------|\n",
        "|         |      |           |             |\n",
        "|         |      |           |             |\n",
        "|         |      |           |             |\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "| palabra | tipo   | categoría   | diccionario        |\n",
        "|---------|--------|-------------|--------------------|\n",
        "| idiota  | ofensa | humillación | ofensa-humillación |\n",
        "| zorra   | ofensa | género      | ofensa-género      |\n",
        "|         |        |             |                    |\n",
        "\n",
        "Puedes [encontrar aquí una copia del diccionario](https://drive.google.com/file/d/1zK214W0pBRYn9lnY_MDJYhEEc6pI3L6F/view?usp=sharing) con la estructura requerida, en formato CSV, para descargar, llenar (opcionalmente) e incorporar a este cuaderno de código.\n",
        "\n",
        "**Nota:** La versión actual de este código requiere importar y usar un diccionario en CSV, aún cuando esté vacío. Si no quieres depurar registros por términos de descarte, puedes solo descargar el diccionario de prueba y subirlo sin agregar términos, solo con los nombres de las columnas indicadas anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2w6zHe_PJtx"
      },
      "outputs": [],
      "source": [
        "# Cargar diccionario con términos de descarte para depuración de términos específicos:\n",
        "rutaDicc = './diccionario-prueba_01.csv'\n",
        "\n",
        "# rutaDicc = 'diccionario-personalizado.csv'\n",
        "dfDescarte = pd.read_csv(rutaDicc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hj8iG14lHb2"
      },
      "source": [
        "**Previsualizar diccionario de descarte importado:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_D1HZSAlHb2"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de diccionario con términos de descarte\n",
        "dfDescarte.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wFtLZbfPJt_"
      },
      "outputs": [],
      "source": [
        "# Revisar el número de filas y columnas en diccionario importado\n",
        "dfDescarte.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-0rQAuqlHb2"
      },
      "outputs": [],
      "source": [
        "# Definir función para cotejar cada registro con términos de descarte, eliminar las coincidencias y agregar su razonamiento en nueva columna\n",
        "def filtrar_registros(df_registros, df_terminos_descarte, colTexto):\n",
        "    # Definir los terminos proscritos que ameritan eliminar el registro\n",
        "    terminos = df_terminos_descarte[\"palabra\"].tolist()\n",
        "    # Compilar expresiones regulares una sola vez\n",
        "    expresiones_regex = [re.compile(r\"(?<!\\S)?(?:\\s|[.,;:?!¡¿]){}(?:\\s|[.,;:?!¡¿])?(?!\\S)\".format(re.escape(termino)), re.IGNORECASE) for termino in terminos]\n",
        "\n",
        "    # Columnas adicionales\n",
        "    df_registros_filtrados = df_registros.copy()\n",
        "    df_registros_filtrados[\"contiene_termino_descarte\"] = False\n",
        "    df_registros_filtrados[\"razon_eliminacion\"] = \"\"\n",
        "\n",
        "    # Filtrar registros\n",
        "    for i in range(df_registros_filtrados.shape[0]):\n",
        "        texto = str(df_registros_filtrados.loc[i, colTexto]).lower().replace(\"á\", \"a\").replace(\"é\", \"e\").replace(\"í\", \"i\").replace(\"ó\", \"o\").replace(\"ú\", \"u\")\n",
        "\n",
        "        # Buscar coincidencias con expresiones regulares\n",
        "        for expresion, razon in zip(expresiones_regex, df_terminos_descarte[\"categoría\"]):\n",
        "            coincidencias = expresion.findall(f\" {texto} \")\n",
        "            if coincidencias:\n",
        "                df_registros_filtrados.loc[i, \"contiene_termino_descarte\"] = True\n",
        "                df_registros_filtrados.loc[i, \"razon_eliminacion\"] = f\"Presencia de términos relacionados a {razon}\"\n",
        "                break\n",
        "\n",
        "    # Eliminar registros que contienen términos de descarte\n",
        "    df_registros_eliminados = df_registros_filtrados[df_registros_filtrados[\"contiene_termino_descarte\"]]\n",
        "    df_registros_filtrados = df_registros_filtrados[~df_registros_filtrados[\"contiene_termino_descarte\"]]\n",
        "\n",
        "    # Eliminar columnas auxiliares\n",
        "    del df_registros_filtrados[\"contiene_termino_descarte\"]\n",
        "    del df_registros_filtrados[\"razon_eliminacion\"]\n",
        "    del df_registros_eliminados[\"contiene_termino_descarte\"]\n",
        "\n",
        "    df_registros_filtrados = df_registros_filtrados.reset_index(drop=True)\n",
        "\n",
        "    return df_registros_filtrados, df_registros_eliminados\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dgm6-sYlHb2"
      },
      "outputs": [],
      "source": [
        "# Ejecutar depuración de registros por palabras de descarte en diccionario\n",
        "datasetSinTerminosProhibidos, datasetRegistrosEliminadosPorDescarte = filtrar_registros(dfFinal, dfDescarte,text_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FP8xV8xioUZb"
      },
      "outputs": [],
      "source": [
        "# Revisar el número de filas y columnas en tabla de registros depurados por palabras de descarte en diccionario\n",
        "datasetSinTerminosProhibidos.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZvLD2OWPJt_"
      },
      "source": [
        "**Tamaño dataset con términos descartados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "119YWtl1Evch",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Revisar el número de filas y columnas en tabla de registros eliminados por palabras de descarte en diccionario\n",
        "datasetRegistrosEliminadosPorDescarte.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasetRegistrosEliminadosPorDescarte.head()"
      ],
      "metadata": {
        "id": "07nzUNQvzhzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzbnrrTKPJuA"
      },
      "outputs": [],
      "source": [
        "#Verificar eliminación de casos específicos (opcional)\n",
        "verificar_termino = \"\"\n",
        "\n",
        "_count = 0\n",
        "for i in datasetRegistrosEliminadosPorDescarte['clean_text']:\n",
        "    words = i.split()\n",
        "    if verificar_termino in words:\n",
        "        _count += 1\n",
        "_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2nOcuD2lpAk"
      },
      "source": [
        "**Eliminar registros repetidas:**\n",
        "\n",
        "Eliminar aquellos registros que contengan una similitud en su redacción mayor a un umbral establecido (por default asignado al 100%), para así buscar eliminar registros con una repetición exacta.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5YSdAyiPJuA"
      },
      "outputs": [],
      "source": [
        "# Definir función para calcular la similitud entre dos listas de palabras\n",
        "def Similarity_Score(list1, list2):\n",
        "    # Inicializar contadores para coincidencias y longitud total\n",
        "    matches = 0\n",
        "    total_length = 0\n",
        "\n",
        "    # Iterar sobre las listas hasta el tamaño de la lista más corta\n",
        "    for i in range(min(len(list1), len(list2))):\n",
        "        # Si las palabras en las mismas posiciones coinciden, incrementar el contador de coincidencias\n",
        "        if list1[i] == list2[i]:\n",
        "            matches += 1\n",
        "        # Incrementar el contador de longitud total\n",
        "        total_length += 1\n",
        "\n",
        "    # Para las posiciones adicionales en la lista más larga, incrementar el contador de longitud total\n",
        "    for i in range(min(len(list1), len(list2)), max(len(list1), len(list2))):\n",
        "        total_length += 1\n",
        "\n",
        "    # Calcular el ratio de coincidencias como la proporción de coincidencias sobre la longitud total\n",
        "    ratio = matches / total_length\n",
        "\n",
        "    return ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS96x0RXPJuA"
      },
      "source": [
        "**Definir función para identificar registros repetidos:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jx-sbH8lrqY"
      },
      "outputs": [],
      "source": [
        "def remove_duplicates_with_threshold(df, column, threshold):\n",
        "    global similarity_score\n",
        "    print(\"Se actualizó\")\n",
        "    indices_to_remove = set()\n",
        "    question_frequency = defaultdict(int) # Diccionario para almacenar la frecuencia de registros similares\n",
        "    discarded_info = defaultdict(list) # Diccionario para almacenar información de registros descartados\n",
        "\n",
        "    # Crear índice invertido para las palabras en los registros\n",
        "    inverted_index = defaultdict(set)\n",
        "    for i, question in enumerate(df[column]):\n",
        "        words = set(question.split())\n",
        "        for word in words:\n",
        "            inverted_index[word].add(i)\n",
        "\n",
        "    print(f\"{len(df[column])} registros en total\")\n",
        "    for i, question in enumerate(df[column]):\n",
        "        if i not in indices_to_remove:\n",
        "            similar_questions_count = 1 # Contador de registros similares para la fila actual\n",
        "            words = set(question.split())\n",
        "            relevant_indices = set()\n",
        "            for word in words:\n",
        "                relevant_indices |= inverted_index[word]\n",
        "\n",
        "            for j in relevant_indices:\n",
        "                if j != i and j not in indices_to_remove:\n",
        "                    registroSinAcentos = question.replace('á', 'a').replace('é','e').replace('í','i').replace('ó','o').replace('ú','u')\n",
        "                    registroSinAcentosEnLista = registroSinAcentos.split(\" \")\n",
        "                    registroAComparar = df[column][j]\n",
        "                    registroACompararSinAcentos = registroAComparar.replace('á', 'a').replace('é','e').replace('í','i').replace('ó','o').replace('ú','u')\n",
        "                    registroACompararEnLista = registroACompararSinAcentos.split(\" \")\n",
        "\n",
        "                    score = Similarity_Score(list(registroSinAcentosEnLista), list(registroACompararEnLista))\n",
        "                    if score >= threshold:\n",
        "                        indices_to_remove.add(j)\n",
        "                        similar_questions_count += 1\n",
        "                        # Almacenar información de la registro descartado\n",
        "                        discarded_info[j].append({'original_index': df['id'][i], 'similarity_score': score})\n",
        "            question_frequency[i] = similar_questions_count # Almacenar la frecuencia de registros similares para la fila actual\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"Van {i} registros revisados...\")\n",
        "\n",
        "    # Eliminar los registros duplicados después de completar el bucle\n",
        "    filtered_df = df.drop(indices_to_remove).reset_index(drop=True)\n",
        "\n",
        "    # Crear DataFrame con registros duplicados\n",
        "    df_removed_duplicates = df.iloc[list(indices_to_remove)]\n",
        "\n",
        "    # Agregar información de registros descartados al DataFrame de registros descartados\n",
        "    id_match = []\n",
        "    similarity_score = []\n",
        "\n",
        "    # Iterar sobre índice de DataFrame\n",
        "    for index in df_removed_duplicates.index:\n",
        "        # Revisar si el índice se encuentra en discarded_info\n",
        "        if index in discarded_info:\n",
        "            # Por cada índice, toma el primer elemento de 'original_index' y 'similarity_score'\n",
        "            id_match.append(discarded_info[index][0]['original_index'])\n",
        "            similarity_score.append(discarded_info[index][0]['similarity_score'])\n",
        "        else:\n",
        "            # Si el índice no está en discarded_info, agregar el valor por default None\n",
        "            id_match.append(None)\n",
        "            similarity_score.append(None)\n",
        "\n",
        "    df_removed_duplicates['id_match'] = id_match\n",
        "    df_removed_duplicates['similarity_score'] = similarity_score\n",
        "\n",
        "    filtered_df['question_frequency_count'] = filtered_df['id'].apply(\n",
        "        lambda x: len(df_removed_duplicates[df_removed_duplicates['id_match'] == x]) + 1)\n",
        "\n",
        "    return filtered_df, df_removed_duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoKv1HZbPJuA"
      },
      "source": [
        "Ejecutar función para **eliminar registros duplicados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roueuL7IPJuB"
      },
      "outputs": [],
      "source": [
        "# Establecer umbral de similitud (porcentaje)\n",
        "threshold = 1 # Se eliminan registros que sean 100% similares en su redacción a alguno ya registrado\n",
        "# Ejecutar elminación de registros repetidos\n",
        "df_filtered, df_removed_duplicates = remove_duplicates_with_threshold(datasetSinTerminosProhibidos, 'sem_text', threshold)#.head(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzqdvJWJNxtu"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de población de registros depurados con la frecuencia de aparición de cada registro respecto a otros registros\n",
        "df_filtered.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2T-iTT6Nxtu"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de registros eliminados por repetición\n",
        "df_removed_duplicates.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4sOXpOtlvXC"
      },
      "outputs": [],
      "source": [
        "# Definir función para agregar la razón de eliminación por repetidos\n",
        "def agregar_razon_eliminacion(df_removed, razon):\n",
        "    df_removed['razon_eliminacion'] = razon\n",
        "    return df_removed\n",
        "\n",
        "df_removed_duplicates = agregar_razon_eliminacion(df_removed_duplicates, 'Redacción repetida respecto a otro registro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDpFQRBGNxuA"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de registros eliminados por repetición con razón de eliminación\n",
        "df_removed_duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y5PWCG9PJuB"
      },
      "outputs": [],
      "source": [
        "# Revisar el número de filas y columnas en tabla de registros eliminados por repetición\n",
        "df_removed_duplicates.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtD-LevA7VCm"
      },
      "source": [
        "**Concatenar tabla con filas eliminadas por términos de descarte en diccionario y por repeticiones:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmXzxCEE7dq-"
      },
      "outputs": [],
      "source": [
        "# Ejecutar concatenación de filas eliminadas por términos en diccionario y repeticiones\n",
        "datasetRegistrosEliminados = pd.concat([datasetRegistrosEliminadosPorDescarte, df_removed_duplicates], axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJMKBy_ANxuA"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de registros eliminados por términos en diccionario y repeticiones con razón de eliminación\n",
        "datasetRegistrosEliminados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf81B8Q2NxuB"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de población de registros depurados\n",
        "df_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-le8Mw8OH5v"
      },
      "source": [
        "## 4. Revisar y exportar datos con registros depurados y eliminados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qEWxvc-bBFGk"
      },
      "outputs": [],
      "source": [
        "# Revisar el número de filas y columnas en tabla de población de registros depurados\n",
        "df_filtered.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5FOsygf8ePw"
      },
      "outputs": [],
      "source": [
        "# Revisar el número de filas y columnas en tabla concatenada de registros eliminados por términos de descarte en diccionarios y repeticiones\n",
        "datasetRegistrosEliminados.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuLWEw8llHb2"
      },
      "source": [
        "**Exportar archivo de datos (en formato CSV) de población de registros depurados a utilizar:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkCxiCz-lHb2"
      },
      "outputs": [],
      "source": [
        "# Ejemplo exportar archivo de datos (CSV) con población de registros depurados\n",
        "df_filtered.to_csv(f\"{nombreProyecto.split('.')[0]}_PoblacionRegistrosDepurados.csv\")\n",
        "\n",
        "print(f\"¡{nombreProyecto.split('.')[0]}_PoblacionRegistrosDepurados.csv descargado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O5EaHqz8rXG"
      },
      "source": [
        "**Exportar archivo de datos (en formato CSV) de registros eliminados por términos de descarte o repeticiones, con su razonamiento correspondiente:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDjyzPwSE2qg"
      },
      "outputs": [],
      "source": [
        "# Ejemplo exportar archivo de datos (CSV) con registros eliminados por términos de descarte en diccionarios y repeticiones\n",
        "datasetRegistrosEliminados.to_csv(f\"{nombreProyecto.split('.')[0]}_RegistrosDescartados.csv\")\n",
        "\n",
        "print(f\"¡{nombreProyecto.split('.')[0]}_RegistrosDescartados.csv descargado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3rKx63aGRWC"
      },
      "source": [
        "## 5. Referencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9oPZQaEekMJ"
      },
      "source": [
        "*   Bird, Steven, Edward Loper & Ewan Klein (2009).\n",
        "Natural Language Processing with Python.  O'Reilly Media Inc.\n",
        "*   Guzmán Falcón, E. (2018). Detección de lenguaje ofensivo en Twitter basada en expansión automática de lexicones (Tesis de Maestría). Instituto Nacional de Astrofísica, Óptica y Electrónica. Recuperado de https://inaoe.repositorioinstitucional.mx/jspui/bitstream/1009/1722/1/GuzmanFE.pdf\n",
        "* Kiss, T., & Strunk, J. (2006). Unsupervised Multilingual Sentence Boundary Detection. Computational Linguistics, 32(4), 485-525. https://doi.org/10.1162/coli.2006.32.4.485"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Créditos"
      ],
      "metadata": {
        "id": "ASBLPGoSNBqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Realizado por el equipo de Signa_Lab ITESO:**\n",
        "\n",
        "- **Programación de cuadernos de código (Python)**:\n",
        "Javier de la Torre Silva, José Luis Almendarez González y Diego Arredondo Ortiz\n",
        "\n",
        "- **Supervisión del desarrollo tecnológico y documentación:**\n",
        "Diego Arredondo Ortiz\n",
        "\n",
        "- **Equipo de Coordinación Signa_Lab ITESO:**\n",
        "Paloma López Portillo Vázquez, Víctor Hugo Ábrego Molina y Eduardo G. de Quevedo Sánchez\n",
        "\n",
        "Mayo, 2024. Instituto Tecnológico y de Estudios Superiores de Occidente (ITESO)\n",
        "Tlaquepaque, Jalisco, México.\n"
      ],
      "metadata": {
        "id": "fdAyvK0mOkwZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S_ePFxKPJuL"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}