{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HtQx5zDeIb0"
      },
      "source": [
        "# **Signa_Lab ITESO:** Generador de *Embbeddings*\n",
        "## **Cuaderno 01:** Limpieza y depuración de texto para procesar *embeddings*\n",
        "\n",
        "Cuaderno de código abierto diseñado para importar cualquier cuerpo de texto separado por filas, en formato CSV o Excel, limpiarlo (*stopwords*, URLs, usuarios y hashtags) y depurarlo desde [diccionarios personalizados](https://drive.google.com/file/d/1zK214W0pBRYn9lnY_MDJYhEEc6pI3L6F/view?usp=drive_link) (opcional) para optimizar la posterior generación de incrustaciones de texto (*embeddings*) de cada fila ([ver cuaderno 02](https://github.com/signalab/generador-embeddings/blob/main/cuadernos/02_Signa_Lab_generador_embeddings_Generar_procesar_reducir_clusterizar_embeddings_01.ipynb)), con ayuda de modelos de lenguaje de la librería [sentence-transformers](https://www.sbert.net/), alojados en repositorios de [HuggingFace](https://huggingface.co/sentence-transformers) (en la nube) o cargados localmente.\n",
        "\n",
        "**\\***Los grupos de celdas marcadas con **asterisco requieren información** antes de seguir adelante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD-wOGZQ5Ki6"
      },
      "source": [
        "## 1. Importar librerías y archivos de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalar e importar librerías:"
      ],
      "metadata": {
        "id": "ReN-KpvrJ-jS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOuliduEPJtw"
      },
      "source": [
        "**Instalar librerías necesarias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kidkEvPHPJtw"
      },
      "outputs": [],
      "source": [
        "# Instalar librerías de Python necesarias\n",
        "\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install difflib\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install scipy\n",
        "!pip install numpy\n",
        "!pip install plotly\n",
        "!pip install time\n",
        "!pip install tqdm\n",
        "!pip install operator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi3kGn_FLC6T"
      },
      "source": [
        "**Importar librerías** necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ3xsq9LK2Sr"
      },
      "outputs": [],
      "source": [
        "# Importar librerías de Python necesarias\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import sys\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from scipy.stats import gaussian_kde\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import math\n",
        "import operator\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Indicar rutas de archivos de datos a importar y nombre de proyecto:"
      ],
      "metadata": {
        "id": "o2-KsJ87KOiZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfsq2zLpPJtx"
      },
      "source": [
        "**Importar y concatenar archivos de datos** (en CSV o Excel):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para cargar archivos a partir de la extensión en su ruta indicada\n",
        "def load_file(path):\n",
        "    if path.endswith('.csv'):\n",
        "        return pd.read_csv(path)\n",
        "    elif path.endswith('.xlsx'):\n",
        "        return pd.read_excel(path)\n",
        "    else:\n",
        "        raise ValueError(\"Formato no compatible. Por favor carga solo archivos .csv or .xlsx.\")\n",
        "\n",
        "# Inicializar lista para alojar todas las rutas y una variable para el DataFrame final, accesible globalmente\n",
        "file_paths = []\n",
        "dfs = []\n",
        "df = None  # DataFrame global\n",
        "\n",
        "# Definir función para añadir un nuevo campo de texto (input) para añadir una ruta de archivo adicional\n",
        "def add_file_input(b=None):\n",
        "    path_input = widgets.Text(value='', placeholder='Escribe la ruta del archivo', description=f'Archivo {len(file_paths) + 1}:')\n",
        "    file_paths.append(path_input)\n",
        "    update_ui()\n",
        "\n",
        "# Definir función para eliminar el último campo de texto (input) para ruta de archivo\n",
        "def remove_file_input(b=None):\n",
        "    if file_paths:\n",
        "        file_paths.pop()\n",
        "        update_ui()\n",
        "\n",
        "# Definir función para procesar y cargar todos los archivos\n",
        "def process_files(b):\n",
        "    global dfs, df\n",
        "    dfs = []  # Vaciar DataFrames\n",
        "\n",
        "    for path_input in file_paths:\n",
        "        path = path_input.value\n",
        "        try:\n",
        "            temp_df = load_file(path)\n",
        "            temp_df['filename'] = path  # Add a column with the filename\n",
        "            dfs.append(temp_df)\n",
        "            print(f\"Nombre de archivo: {path}\")\n",
        "            print(f\"Filas/Columnas (shape): {temp_df.shape}\")\n",
        "        except ValueError as e:\n",
        "            print(f\"Error al cargar el archivo {path}: {e}\")\n",
        "            return\n",
        "\n",
        "    if dfs:\n",
        "        df = pd.concat(dfs, ignore_index=True)  # Concatenate all DataFrames\n",
        "        print(\"\\n¡Se cargaron todos los archivos!\")\n",
        "        print(f\"Filas/Columnas (shape) de DataFrame creado: {df.shape}\")\n",
        "\n",
        "# Campo de texto (input) para indicar nombre del proyecto (para integrarse en nombres de archivos a exportar)\n",
        "project_name = widgets.Text(value='', placeholder='Escribe el nombre del proyecto (corto y sin espacios)', description='Nombre proyecto:')\n",
        "\n",
        "# Botones para añadir y eliminar archivos\n",
        "add_button = widgets.Button(description=\"Añadir archivo\",  button_style='')\n",
        "remove_button = widgets.Button(description=\"Eliminar archivo\",  button_style='warning')\n",
        "load_button = widgets.Button(description=\"Cargar archivos\",  button_style='primary')\n",
        "\n",
        "add_button.on_click(add_file_input)\n",
        "remove_button.on_click(remove_file_input)\n",
        "load_button.on_click(process_files)\n",
        "\n",
        "# Definir función para actualizar UI\n",
        "def update_ui():\n",
        "    clear_output()\n",
        "    display(project_name)\n",
        "    for path_input in file_paths:\n",
        "        display(path_input)\n",
        "    display(widgets.HBox([add_button, remove_button]))\n",
        "    display(load_button)\n",
        "\n",
        "# Inicializar UI con un campo de texto (input) para ruta de archivo\n",
        "add_file_input()\n"
      ],
      "metadata": {
        "id": "hEtma1lgB6Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previsualizar datos importados:"
      ],
      "metadata": {
        "id": "O9kQer1lRAg5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HptxpF1lHby"
      },
      "source": [
        "**Previsualizar tabla** con todos los registros importados:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsualizar dataframe con CSVs importados\n",
        "display(df)\n",
        "print(f\"Filas/Columnas (shape) en registros importados: {df.shape}\")\n"
      ],
      "metadata": {
        "id": "IM2Rv28GEHOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exportar copia en CSV con registros importados (o concatenados):**"
      ],
      "metadata": {
        "id": "9xaQzFYoasLs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nheXTnM7PJt0"
      },
      "outputs": [],
      "source": [
        "# Exportar archivo CSV con tabla de registros importados (y concatenados, en el caso de múltiples archivos)\n",
        "df.to_csv(f\"{project_name.value}_registros-importados.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hPbcqVvlHbz"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Limpieza de registros importados"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generar identificadores únicos (IDs) por registro:"
      ],
      "metadata": {
        "id": "l8Fx8oCP7tYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para asignar IDs únicos a cada fila en el data frame indicado como parámetro, comenzando desde '1000001'.\n",
        "def assign_unique_ids(df):\n",
        "    # Inicializar contador para IDs\n",
        "    id_counter = 1000001\n",
        "\n",
        "    # Crear copia de data frame original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Iterar a través de las filas del data frame\n",
        "    for index, _ in enumerate(df_copy.index):\n",
        "        # Dar formato a ID con ceros adicionales e incorporarlo al data frame\n",
        "        formatted_id = str(id_counter).zfill(7)  # Se asegura de que sea un ID de 7 dígitos, agregando ceros cuando sea necesario\n",
        "        df_copy.loc[index, 'id'] = formatted_id\n",
        "\n",
        "        # Incrementar el contador del ID para la siguiente iteración\n",
        "        id_counter += 1\n",
        "\n",
        "    return df_copy"
      ],
      "metadata": {
        "id": "EhuSrvzjr1Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar función para asignar IDs a cada registro y previsualizar tabla\n",
        "if __name__ == \"__main__\":\n",
        "    # Invocar la función con data frame de trabajo\n",
        "    df_ids = assign_unique_ids(df)\n",
        "\n",
        "# Sobreescribir data frame con nueva tabla con IDs generados\n",
        "df = df_ids\n",
        "df"
      ],
      "metadata": {
        "id": "UyPVvBIWsVzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVfoXAtilHbz"
      },
      "source": [
        "### *Elegir criterios para limpieza de texto sin aporte semántico:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para limpiar usuarios, hashtags y URLs\n",
        "def limpiar_texto(texto, eliminar_usuarios, eliminar_hashtags, eliminar_urls, regex_personalizado):\n",
        "    # Eliminar usuarios si está activado\n",
        "    if eliminar_usuarios:\n",
        "        texto = re.sub(r\"(?<!\\w)@(\\w+)(?!\\w)\", \"\", texto)\n",
        "\n",
        "    # Eliminar hashtags si está activado\n",
        "    if eliminar_hashtags:\n",
        "        texto = re.sub(r\"(?<!\\w)#(\\w+)(?!\\w)\", \"\", texto)\n",
        "\n",
        "    # Eliminar URLs si está activado\n",
        "    if eliminar_urls:\n",
        "        texto = re.sub(r\"(http|https|ftp)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\", \"\", texto)\n",
        "        texto = texto.lstrip(\". \")\n",
        "\n",
        "    # Aplicar regex personalizado si se proporciona\n",
        "    if regex_personalizado:\n",
        "        texto = re.sub(regex_personalizado, \"\", texto)\n",
        "\n",
        "    return texto.strip()\n",
        "\n",
        "# Definir función para agregar nueva columna con texto limpio (clean_text)\n",
        "def agregarCleanTextADf(df, colText, eliminar_usuarios, eliminar_hashtags, eliminar_urls, regex_personalizado):\n",
        "    dfW = df.copy()\n",
        "    dfW[\"clean_text\"] = None\n",
        "\n",
        "    for index, row in dfW.iterrows():\n",
        "        text = str(row[colText])\n",
        "        cleaned_text = limpiar_texto(text, eliminar_usuarios, eliminar_hashtags, eliminar_urls, regex_personalizado)\n",
        "        dfW.at[index, \"clean_text\"] = cleaned_text\n",
        "\n",
        "    return dfW\n",
        "\n",
        "# Definir widgets de IPyWidgets para la UI interactiva\n",
        "eliminar_usuarios = widgets.Checkbox(value=True, description='Eliminar usuarios @')\n",
        "eliminar_hashtags = widgets.Checkbox(value=True, description='Eliminar hashtags #')\n",
        "eliminar_urls = widgets.Checkbox(value=True, description='Eliminar URLs')\n",
        "regex_input = widgets.Text(value='', description='Otro (Regex):', placeholder='Escribe un regex opcional')\n",
        "\n",
        "# Nuevo campo de texto para ingresar la columna que contiene el texto a limpiar\n",
        "text_column_input = widgets.Text(value='title', description='Columna:', placeholder='Escribe el nombre de la columna')\n",
        "\n",
        "# Botón para ejecutar la limpieza\n",
        "boton_limpiar = widgets.Button(description=\"Limpiar texto\", button_style='warning')\n",
        "\n",
        "# Caja de texto para mostrar el resultado\n",
        "output_resultado = widgets.Output()\n",
        "\n",
        "# # Definir un DataFrame de ejemplo para pruebas\n",
        "# df = pd.DataFrame({\n",
        "#     'title': ['Este es un ejemplo #hashtag @usuario http://example.com',\n",
        "#               'Otro tweet con más texto @usuario2 #otrohashtag https://test.com']\n",
        "# })\n",
        "\n",
        "# Función que se ejecuta al hacer clic en el botón de limpiar texto\n",
        "def ejecutar_limpieza(b):\n",
        "    global dfCleanText  # Hacer que dfCleanText sea accesible globalmente\n",
        "    with output_resultado:\n",
        "        output_resultado.clear_output()  # Limpiar cualquier salida previa\n",
        "\n",
        "        # Obtener el nombre de la columna ingresado por el usuario\n",
        "        text_column = text_column_input.value\n",
        "\n",
        "        # Verificar si la columna existe en el DataFrame\n",
        "        if text_column not in df.columns:\n",
        "            print(f\"Error: La columna '{text_column}' no existe en el DataFrame.\")\n",
        "            return\n",
        "\n",
        "        # Ejecutar la función de limpieza sobre el DataFrame seleccionado\n",
        "        dfCleanText = agregarCleanTextADf(\n",
        "            df, text_column,\n",
        "            eliminar_usuarios.value,\n",
        "            eliminar_hashtags.value,\n",
        "            eliminar_urls.value,\n",
        "            regex_input.value\n",
        "        )\n",
        "\n",
        "        # Mostrar el DataFrame con la nueva columna 'clean_text'\n",
        "        print(\"Texto limpio aplicado. DataFrame actualizado globalmente como 'dfCleanText'.\")\n",
        "        display(dfCleanText)\n",
        "\n",
        "# Conectar el botón con la función de limpieza\n",
        "boton_limpiar.on_click(ejecutar_limpieza)\n",
        "\n",
        "# Desplegar los widgets en pantalla\n",
        "display(text_column_input, eliminar_usuarios, eliminar_hashtags, eliminar_urls, regex_input, boton_limpiar, output_resultado)\n"
      ],
      "metadata": {
        "id": "9LINWys5czcv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f86YTwWJlHb0"
      },
      "source": [
        "### \\*Elegir idioma de palabras vacías (*stop words*) a eliminar:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMi4M_w-lHb0"
      },
      "source": [
        "Elegir **idioma de** (desde diccionarios de NLTK), **eliminar *stop words*** y **agregar columna sem_text** con texto depurado:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Definir función para eliminar stopwords y signos de puntuación\n",
        "def delete_stopwords(texto, stopwords_list):\n",
        "    # Tokenizar el texto\n",
        "    tokens = nltk.word_tokenize(texto)\n",
        "\n",
        "    # Eliminar signos de puntuación\n",
        "    tokens = [token for token in tokens if token.isalnum()]\n",
        "\n",
        "    # Eliminar stop words\n",
        "    tokens = [token for token in tokens if token.lower() not in stopwords_list]\n",
        "\n",
        "    # Convertir la lista de tokens a un string\n",
        "    texto_limpio = \" \".join(tokens)\n",
        "\n",
        "    return texto_limpio.strip()\n",
        "\n",
        "# Definir función para eliminar palabras vacías y agregar nueva columna con el resultado (sem_text)\n",
        "def agregarSemTextADf(df, colText, stopwords_list):\n",
        "    dfW = df.copy()\n",
        "    dfW[\"sem_text\"] = None\n",
        "\n",
        "    for index, row in dfW.iterrows():\n",
        "        text = row[colText]\n",
        "        cleaned_text = delete_stopwords(text, stopwords_list)\n",
        "        dfW.at[index, \"sem_text\"] = cleaned_text\n",
        "\n",
        "    return dfW\n",
        "\n",
        "# Lista de idiomas disponibles en NLTK\n",
        "idiomas_stopwords = [\n",
        "    'arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french',\n",
        "    'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali',\n",
        "    'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish',\n",
        "    'swedish', 'tajik', 'turkish'\n",
        "]\n",
        "\n",
        "# Crear dropdown para seleccionar idioma\n",
        "dropdown_idioma = widgets.Dropdown(\n",
        "    options=idiomas_stopwords,\n",
        "    value='spanish',  # Valor por defecto\n",
        "    description='Idioma:'\n",
        ")\n",
        "\n",
        "# Botón para eliminar stopwords\n",
        "boton_eliminar_stopwords = widgets.Button(description=\"Eliminar stopwords\", button_style='warning')\n",
        "\n",
        "# Output para mostrar el resultado\n",
        "output_resultado_stopwords = widgets.Output()\n",
        "\n",
        "# Función que se ejecuta al hacer clic en el botón de eliminar stopwords\n",
        "def ejecutar_eliminacion_stopwords(b):\n",
        "    global dfClean  # Hacer que dfClean sea accesible globalmente\n",
        "    with output_resultado_stopwords:\n",
        "        output_resultado_stopwords.clear_output()  # Limpiar cualquier salida previa\n",
        "\n",
        "        # Obtener el idioma seleccionado\n",
        "        idioma_seleccionado = dropdown_idioma.value\n",
        "\n",
        "        # Descargar stopwords del idioma seleccionado\n",
        "        stopwords_list = stopwords.words(idioma_seleccionado)\n",
        "\n",
        "        # Verificar si la columna 'clean_text' existe en el DataFrame\n",
        "        if 'clean_text' not in dfCleanText.columns:\n",
        "            print(\"Error: La columna 'clean_text' no existe en el DataFrame.\")\n",
        "            return\n",
        "\n",
        "        # Ejecutar la función para eliminar stopwords y agregar la nueva columna 'sem_text'\n",
        "        dfClean = agregarSemTextADf(dfCleanText, 'clean_text', stopwords_list)\n",
        "\n",
        "        # Mostrar el DataFrame con la nueva columna 'sem_text'\n",
        "        print(f\"Stopwords en {idioma_seleccionado} eliminadas. DataFrame actualizado globalmente como 'dfClean'.\")\n",
        "        display(dfClean)\n",
        "\n",
        "# Conectar el botón con la función de eliminación de stopwords\n",
        "boton_eliminar_stopwords.on_click(ejecutar_eliminacion_stopwords)\n",
        "\n",
        "# Desplegar los widgets en pantalla\n",
        "display(dropdown_idioma, boton_eliminar_stopwords, output_resultado_stopwords)\n",
        "\n",
        "# Exportar archivo CSV con tabla completa de registros importados con IDs y texto sin palabras vacías (sem_text)\n",
        "dfClean.to_csv(f\"{project_name.value}_registros-semtext.csv\")\n"
      ],
      "metadata": {
        "id": "kQtNOs0Ulz2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHNNKJdTlHb1"
      },
      "source": [
        "## 3. Depuración de registros desde diccionarios personalizados (con términos de filtrado o descarte)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Definir uso de de diccionario de depuración:"
      ],
      "metadata": {
        "id": "KL71R1c4O6cp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t-7ye11YwN2"
      },
      "source": [
        "A partir de diccionarios personalizados con términos de descarte o filtración, elige depurar o mantener, respectivamente, registros que los contengan.\n",
        "\n",
        "El diccionario debe cargarse en formato CSV y contener, al menos, los siguientes campos:\n",
        "\n",
        "| palabra | tipo | categoría | diccionario |\n",
        "|---------|------|-----------|-------------|\n",
        "|         |      |           |             |\n",
        "|         |      |           |             |\n",
        "|         |      |           |             |\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "| palabra | tipo   | categoría   | diccionario        |\n",
        "|---------|--------|-------------|--------------------|\n",
        "| idiota  | ofensa | humillación | ofensa-humillación |\n",
        "| zorra   | ofensa | género      | ofensa-género      |\n",
        "|         |        |             |                    |\n",
        "\n",
        "Puedes [encontrar aquí una copia del diccionario](https://drive.google.com/file/d/1zK214W0pBRYn9lnY_MDJYhEEc6pI3L6F/view?usp=sharing) con la estructura requerida, en formato CSV, para descargar, llenar e incorporar a este cuaderno de código.\n",
        "\n",
        "**Nota:** La versión actual de este cuaderno de código permite elegir entre las siguientes opciones sobre la carga de diccionarios:\n",
        "\n",
        "\n",
        "- ***No cargar diccionario:*** Ignorar esta funcionalidad y no usar diccionarios.\n",
        "- ***Diccionario de descarte (negativo):*** Se eliminarán todos los registros que mencionen alguno de esos términos.\n",
        "- ***Diccionario de filtrado (positivo):*** Se mantendrán solo los registros que mencionen alguno de esos términos."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dropdown widget for dictionary usage selection\n",
        "cargar_diccionario = widgets.Dropdown(\n",
        "    options=[\"No cargar diccionario\", \"Diccionario de descarte (negativo)\", \"Diccionario de filtrado (positivo)\"],\n",
        "    value=\"No cargar diccionario\",\n",
        "    description=\"Elegir uso:\"\n",
        ")\n",
        "\n",
        "# Create the text input widget for file path\n",
        "rutaDicc = widgets.Text(\n",
        "    value=\"\",\n",
        "    placeholder=\"Escribir ruta a archivo CSV\",\n",
        "    description=\"Ruta:\"\n",
        ")\n",
        "\n",
        "# Create the \"Aceptar\" button\n",
        "accept_button = widgets.Button(description=\"Aceptar\")\n",
        "\n",
        "# Create an output widget to display results or errors\n",
        "output = widgets.Output()\n",
        "\n",
        "# Function to load the dictionary based on user input\n",
        "def load_dictionary(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "            try:\n",
        "                global dfDiccionario\n",
        "                dfDiccionario = pd.read_csv(rutaDicc.value)\n",
        "                print(f\"Archivo de diccionario '{rutaDicc.value}' cargado exitosamente.\")\n",
        "                display(dfDiccionario.head())  # Show the first few rows of the loaded dictionary\n",
        "            except FileNotFoundError:\n",
        "                print(f\"El archivo de diccionario '{rutaDicc.value}' no fue encontrado. Por favor, verifique la ruta.\")\n",
        "        else:\n",
        "            dfDiccionario = None\n",
        "            print(\"No se cargó ningún diccionario.\")\n",
        "\n",
        "# Link the button to the load function\n",
        "accept_button.on_click(load_dictionary)\n",
        "\n",
        "# Display the widgets\n",
        "display(cargar_diccionario, rutaDicc, accept_button, output)\n"
      ],
      "metadata": {
        "id": "AuzxsTVCg-uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar uso de diccionario elegido\n",
        "print(f\"Uso de diccionario elegido: {cargar_diccionario.value}\")"
      ],
      "metadata": {
        "id": "24zZKqwCSgo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aplicar filtrado por diccionario y previsualizar resultados (opcional, solo si se cargó algún diccionario):"
      ],
      "metadata": {
        "id": "MS2u9S1B-W2_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hj8iG14lHb2"
      },
      "source": [
        "**Previsualizar diccionario de descarte importado:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_D1HZSAlHb2"
      },
      "outputs": [],
      "source": [
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  # Previsualizar tabla de diccionario cargado (en caso de haber elegido utilizar diccionarios)\n",
        "  try:\n",
        "      display(dfDiccionario.head())  # Previsualizar primeros registros de diccionario\n",
        "      print(f\"\\nFilas y columnas en diccionario cargado:\")  # Verificar número de filas y columnas en diccionario\n",
        "      print(f\"Shape: {dfDiccionario.shape}\")  # Verificar número de filas y columnas en diccionario\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filtrar registros a partir de diccionario cargado y modalidad de depuración (positiva o negativa):**"
      ],
      "metadata": {
        "id": "JVUqxmIe6LHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filtrar_registros(df_registros, df_terminos, colTexto, cargar_diccionario):\n",
        "    # Definir los términos del diccionario\n",
        "    terminos = df_terminos[\"palabra\"].tolist()\n",
        "    # Compilar expresiones regulares una sola vez\n",
        "    expresiones_regex = [\n",
        "        re.compile(r\"(?<!\\S)?(?:\\s|[.,;:?!¡¿]){}(?:\\s|[.,;:?!¡¿])?(?!\\S)\".format(re.escape(termino)), re.IGNORECASE)\n",
        "        for termino in terminos\n",
        "    ]\n",
        "\n",
        "    # Copiar el dataframe de registros y agregar columnas auxiliares\n",
        "    df_registros_filtrados = df_registros.copy()\n",
        "    df_registros_filtrados[\"contiene_termino\"] = False\n",
        "    df_registros_filtrados[\"razon_eliminacion\"] = \"\"\n",
        "\n",
        "    # Filtrar registros\n",
        "    for i in range(df_registros_filtrados.shape[0]):\n",
        "        texto = str(df_registros_filtrados.loc[i, colTexto]).lower().replace(\"á\", \"a\").replace(\"é\", \"e\").replace(\"í\", \"i\").replace(\"ó\", \"o\").replace(\"ú\", \"u\")\n",
        "\n",
        "        # Buscar coincidencias con expresiones regulares\n",
        "        for expresion, razon in zip(expresiones_regex, df_terminos[\"categoría\"]):\n",
        "            coincidencias = expresion.findall(f\" {texto} \")\n",
        "            if coincidencias:\n",
        "                df_registros_filtrados.loc[i, \"contiene_termino\"] = True\n",
        "                df_registros_filtrados.loc[i, \"razon_eliminacion\"] = f\"Presencia de términos relacionados a {razon}\"\n",
        "                break\n",
        "            else:\n",
        "                df_registros_filtrados.loc[i, \"contiene_termino\"] = False\n",
        "                df_registros_filtrados.loc[i, \"razon_eliminacion\"] = f\"Ausencia de términos relacionados a {razon}\"\n",
        "\n",
        "    # Revisar uso elegido del diccionario, para descarte (negativo) o filtrado (positivo)\n",
        "    if cargar_diccionario == \"Diccionario de descarte (negativo)\":\n",
        "        # Mantener registros que no contengan términos del diccionario (negativo)\n",
        "        df_registros_filtrados_final = df_registros_filtrados[~df_registros_filtrados[\"contiene_termino\"]]\n",
        "        df_registros_eliminados = df_registros_filtrados[df_registros_filtrados[\"contiene_termino\"]]\n",
        "    elif cargar_diccionario == \"Diccionario de filtrado (positivo)\":\n",
        "        # Mantener registros que sí contengan al menos un término del diccionario (positivo)\n",
        "        df_registros_filtrados_final = df_registros_filtrados[df_registros_filtrados[\"contiene_termino\"]]\n",
        "        df_registros_eliminados = df_registros_filtrados[~df_registros_filtrados[\"contiene_termino\"]]\n",
        "\n",
        "    # Eliminar columnas auxiliares para tabla con registros depurados\n",
        "    df_registros_filtrados_final = df_registros_filtrados_final.drop(columns=[\"contiene_termino\", \"razon_eliminacion\"])\n",
        "\n",
        "    df_registros_filtrados_final = df_registros_filtrados_final.reset_index(drop=True)\n",
        "    df_registros_eliminados = df_registros_eliminados.reset_index(drop=True)\n",
        "\n",
        "    return df_registros_filtrados_final, df_registros_eliminados\n"
      ],
      "metadata": {
        "id": "R6lyxcaMHx6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dgm6-sYlHb2"
      },
      "outputs": [],
      "source": [
        "# Definir el nombre de la columna que contiene el texto, si no está ya definido\n",
        "# Asegúrate de que esta columna exista en el DataFrame dfClean\n",
        "if 'text_column' not in globals():\n",
        "    text_column = 'sem_text'  # Cambia 'sem_text' por el nombre correcto de la columna\n",
        "\n",
        "# Ejecutar depuración de registros por diccionario cargado\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "    try:\n",
        "        # Asegúrate de que dfDiccionario esté definido y cargado antes de la función\n",
        "        if 'dfDiccionario' in globals() and isinstance(dfDiccionario, pd.DataFrame):\n",
        "            df_depurados_dicc, df_eliminados_dicc = filtrar_registros(dfClean, dfDiccionario, text_column, cargar_diccionario.value)\n",
        "        else:\n",
        "            print(\"El diccionario no está cargado o no es un DataFrame válido.\")\n",
        "    except NameError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "else:\n",
        "    print(\"No se cargó ningún diccionario...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Previsualizar registros depurados y eliminados (opcional):**"
      ],
      "metadata": {
        "id": "U4dnWce36Uek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsualizar tabla de registros depurados (conservados) y verificar número de filas y columna\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  try:\n",
        "    display(df_depurados_dicc)\n",
        "    print(f\"\\nFilas/Columnas (shape) en registros conservados por diccionario: {df_depurados_dicc.shape}\")\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")"
      ],
      "metadata": {
        "id": "RGDYqSyOlZVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsualizar tabla de registros eliminados y verificar su número de filas y columnas\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  try:\n",
        "    display(df_eliminados_dicc)\n",
        "    print(f\"\\nFilas/Columnas (shape) en registros eliminados por diccionario: {df_eliminados_dicc.shape}\")\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")"
      ],
      "metadata": {
        "id": "8qHoZTZV6bZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzbnrrTKPJuA"
      },
      "outputs": [],
      "source": [
        "# # Verificar eliminación de casos específicos en registros eliminados o conservados (opcional)\n",
        "\n",
        "# # Indicar palabra a buscar\n",
        "# if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "#   verificar_termino = \"\"\n",
        "\n",
        "#   _count = 0\n",
        "#   # Buscar palabra en registros conservados\n",
        "#   for i in df_depurados_dicc['sem_text']:\n",
        "\n",
        "#   # Buscar palabra en registros eliminados\n",
        "#   # for i in df_eliminados_dicc['sem_text']:\n",
        "\n",
        "#       words = i.split()\n",
        "#       if verificar_termino in words:\n",
        "#           _count += 1\n",
        "#   print(_count)\n",
        "# else:\n",
        "#   print(\"No se cargó ningún diccionario...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Revisar, contar y eliminar registros repetidos"
      ],
      "metadata": {
        "id": "94TJeKlg_GlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Definir umbral de similitud y aplicar función para eliminar redacciones repetidas:"
      ],
      "metadata": {
        "id": "PncydP2EPWYU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2nOcuD2lpAk"
      },
      "source": [
        "**Eliminar registros repetidos:**\n",
        "\n",
        "Eliminar aquellos registros que contengan una similitud en su redacción mayor a un umbral establecido (por default asignado al 100%, con valor de 1), para así buscar eliminar registros con una repetición exacta.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5YSdAyiPJuA"
      },
      "outputs": [],
      "source": [
        "# Definir función para calcular la similitud entre dos listas de palabras\n",
        "def Similarity_Score(list1, list2):\n",
        "    # Inicializar contadores para coincidencias y longitud total\n",
        "    matches = 0\n",
        "    total_length = 0\n",
        "\n",
        "    # Iterar sobre las listas hasta el tamaño de la lista más corta\n",
        "    for i in range(min(len(list1), len(list2))):\n",
        "        # Si las palabras en las mismas posiciones coinciden, incrementar el contador de coincidencias\n",
        "        if list1[i] == list2[i]:\n",
        "            matches += 1\n",
        "        # Incrementar el contador de longitud total\n",
        "        total_length += 1\n",
        "\n",
        "    # Para las posiciones adicionales en la lista más larga, incrementar el contador de longitud total\n",
        "    for i in range(min(len(list1), len(list2)), max(len(list1), len(list2))):\n",
        "        total_length += 1\n",
        "\n",
        "    # Calcular el ratio de coincidencias como la proporción de coincidencias sobre la longitud total\n",
        "    ratio = matches / total_length\n",
        "\n",
        "    return ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS96x0RXPJuA"
      },
      "source": [
        "**Definir función para identificar registros repetidos:**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para eliminar duplicados con umbral\n",
        "def remove_duplicates_with_threshold(df, colText, threshold):\n",
        "# def remove_duplicates_with_threshold(df, column, threshold):\n",
        "    global similarity_score\n",
        "    global df_removed_duplicates\n",
        "    print(\"Se actualizó\")\n",
        "    indices_to_remove = set()\n",
        "    sentence_frequency = defaultdict(int) # Diccionario para almacenar la frecuencia de registros similares\n",
        "    discarded_info = defaultdict(list) # Diccionario para almacenar información de registros descartados\n",
        "\n",
        "    # Crear índice invertido para las palabras en los registros\n",
        "    inverted_index = defaultdict(set)\n",
        "    for i, sentence in enumerate(df[colText]):\n",
        "        words = set(sentence.split())\n",
        "        for word in words:\n",
        "            inverted_index[word].add(i)\n",
        "\n",
        "    print(f\"{len(df[colText])} registros en total\")\n",
        "    for i, sentence in enumerate(df[colText]):\n",
        "        if i not in indices_to_remove:\n",
        "            similar_sentences_count = 1 # Contador de registros similares para la fila actual\n",
        "            words = set(sentence.split())\n",
        "            relevant_indices = set()\n",
        "            for word in words:\n",
        "                relevant_indices |= inverted_index[word]\n",
        "\n",
        "            for j in relevant_indices:\n",
        "                if j != i and j not in indices_to_remove:\n",
        "                    registroSinAcentos = sentence.replace('á', 'a').replace('é','e').replace('í','i').replace('ó','o').replace('ú','u')\n",
        "                    registroSinAcentosEnLista = registroSinAcentos.split(\" \")\n",
        "                    registroAComparar = df[colText][j]\n",
        "                    registroACompararSinAcentos = registroAComparar.replace('á', 'a').replace('é','e').replace('í','i').replace('ó','o').replace('ú','u')\n",
        "                    registroACompararEnLista = registroACompararSinAcentos.split(\" \")\n",
        "\n",
        "                    score = Similarity_Score(list(registroSinAcentosEnLista), list(registroACompararEnLista))\n",
        "                    if score >= threshold:\n",
        "                        indices_to_remove.add(j)\n",
        "                        similar_sentences_count += 1\n",
        "                        # Almacenar información de la registro descartado\n",
        "                        discarded_info[j].append({'original_index': df['id'][i], 'similarity_score': score})\n",
        "            sentence_frequency[i] = similar_sentences_count # Almacenar la frecuencia de registros similares para la fila actual\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"Van {i} registros revisados...\")\n",
        "\n",
        "    # Eliminar los registros duplicados después de completar el bucle\n",
        "    filtered_df = df.drop(indices_to_remove).reset_index(drop=True)\n",
        "\n",
        "    # Crear DataFrame con registros duplicados\n",
        "    df_removed_duplicates = df.iloc[list(indices_to_remove)]\n",
        "\n",
        "    # Agregar información de registros descartados al DataFrame de registros descartados\n",
        "    id_match = []\n",
        "    similarity_score = []\n",
        "\n",
        "    # Iterar sobre índice de DataFrame\n",
        "    for index in df_removed_duplicates.index:\n",
        "        # Revisar si el índice se encuentra en discarded_info\n",
        "        if index in discarded_info:\n",
        "            # Por cada índice, toma el primer elemento de 'original_index' y 'similarity_score'\n",
        "            id_match.append(discarded_info[index][0]['original_index'])\n",
        "            similarity_score.append(discarded_info[index][0]['similarity_score'])\n",
        "        else:\n",
        "            # Si el índice no está en discarded_info, agregar el valor por default None\n",
        "            id_match.append(None)\n",
        "            similarity_score.append(None)\n",
        "\n",
        "    df_removed_duplicates['id_match'] = id_match\n",
        "    df_removed_duplicates['similarity_score'] = similarity_score\n",
        "\n",
        "    filtered_df['sentence_frequency_count'] = filtered_df['id'].apply(\n",
        "        lambda x: len(df_removed_duplicates[df_removed_duplicates['id_match'] == x]) + 1)\n",
        "\n",
        "    return filtered_df, df_removed_duplicates\n",
        "\n",
        "# Slider para seleccionar el umbral de similitud (0 a 1)\n",
        "slider_threshold = widgets.FloatSlider(\n",
        "    value=1,  # Valor inicial\n",
        "    min=0,    # Mínimo\n",
        "    max=1,    # Máximo\n",
        "    step=0.01,  # Incrementos de 0.01\n",
        "    description='Umbral (0-1):',\n",
        "    readout_format='.2f'\n",
        ")\n",
        "\n",
        "# Texto que indica que el umbral es en porcentaje de similitud (0% a 100%)\n",
        "label_threshold = widgets.Label(value=\"Selecciona el umbral de similitud (0% a 100%)\")\n",
        "\n",
        "# Botón para ejecutar la eliminación de duplicados\n",
        "boton_eliminar_duplicados = widgets.Button(description=\"Eliminar duplicados\", button_style='warning')\n",
        "\n",
        "# Output para mostrar el resultado\n",
        "output_resultado_duplicados = widgets.Output()\n",
        "\n",
        "# Función que se ejecuta al hacer clic en el botón\n",
        "def ejecutar_eliminacion_duplicados(b):\n",
        "    global df_depurados_final  # Definir df_depurados_final como variable global\n",
        "    with output_resultado_duplicados:\n",
        "        output_resultado_duplicados.clear_output()  # Limpiar cualquier salida previa\n",
        "\n",
        "        # Obtener el valor del umbral del slider\n",
        "        threshold_value = slider_threshold.value\n",
        "\n",
        "        # Verificar si se está cargando un diccionario\n",
        "        if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "            df_depurados_final, df_removed_duplicates = remove_duplicates_with_threshold(df_depurados_dicc, 'sem_text', threshold_value)\n",
        "        else:\n",
        "            df_depurados_final, df_removed_duplicates = remove_duplicates_with_threshold(dfClean, 'sem_text', threshold_value)\n",
        "            print(\"No se cargó ningún diccionario...\")\n",
        "\n",
        "        # Mostrar mensaje de éxito\n",
        "        print(f\"Eliminación de duplicados ejecutada con un umbral de {threshold_value*100:.2f}% de similitud.\")\n",
        "        display(df_depurados_final)\n",
        "\n",
        "# Conectar el botón con la función de eliminación de duplicados\n",
        "boton_eliminar_duplicados.on_click(ejecutar_eliminacion_duplicados)\n",
        "\n",
        "# Desplegar los widgets en pantalla\n",
        "display(label_threshold, slider_threshold, boton_eliminar_duplicados, output_resultado_duplicados)\n"
      ],
      "metadata": {
        "id": "zIDtmsMjq9E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previsualizar resultados de conteo y depuración de registros repetidos:"
      ],
      "metadata": {
        "id": "H3dvOzzoPeVs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4sOXpOtlvXC"
      },
      "outputs": [],
      "source": [
        "# Definir función para agregar la razón de eliminación por repetidos\n",
        "def agregar_razon_eliminacion(df_removed, razon):\n",
        "    df_removed['razon_eliminacion'] = razon\n",
        "    return df_removed\n",
        "\n",
        "df_removed_duplicates = agregar_razon_eliminacion(df_removed_duplicates, 'Redacción repetida respecto a otro registro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDpFQRBGNxuA"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de registros eliminados por repetición con razón de eliminación\n",
        "df_removed_duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y5PWCG9PJuB"
      },
      "outputs": [],
      "source": [
        "# Revisar el número de filas y columnas en tabla de registros eliminados por repetición\n",
        "df_removed_duplicates.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-le8Mw8OH5v"
      },
      "source": [
        "## 5. Revisar y exportar datos con registros depurados y eliminados"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consolidar y previsualizar tablas finales de registros depurados y eliminados:"
      ],
      "metadata": {
        "id": "m0Hh2sqiP4Ck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtD-LevA7VCm"
      },
      "source": [
        "**Consolidar tabla de registros eliminados, concatenando tablas de eliminados por diccionario (en caso de haberse cargado) y por repeticiones:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmXzxCEE7dq-"
      },
      "outputs": [],
      "source": [
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  try:\n",
        "    # Ejecutar concatenación de filas eliminadas por términos en diccionario y repeticiones\n",
        "    df_eliminados_final = pd.concat([df_eliminados_dicc, df_removed_duplicates], axis=0)\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  df_eliminados_final = df_removed_duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJMKBy_ANxuA"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de registros totales eliminados (por términos en diccionario y repeticiones), con razón de eliminación\n",
        "display(df_eliminados_final)\n",
        "print(f\"Filas/Columnas (shape) en registros totales eliminados: {df_eliminados_final.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf81B8Q2NxuB"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de registros totales depurados (conservados)\n",
        "display(df_depurados_final)\n",
        "print(f\"Filas/Columnas (shape) en registros totales conservados: {df_depurados_final.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exportar archivos CSV con tablas finales de registros depurados y elminados:"
      ],
      "metadata": {
        "id": "Qi6zLeREQCLZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuLWEw8llHb2"
      },
      "source": [
        "**Exportar archivo de datos (en formato CSV) de población de registros depurados a utilizar:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkCxiCz-lHb2"
      },
      "outputs": [],
      "source": [
        "# Ejemplo exportar archivo de datos (CSV) con población de registros depurados\n",
        "df_depurados_final.to_csv(f\"{project_name.value}_registros-depurados.csv\")\n",
        "\n",
        "print(f\"¡{project_name.value}_registros-depurados.csv descargado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O5EaHqz8rXG"
      },
      "source": [
        "**Exportar archivo de datos (en formato CSV) de registros eliminados por términos de descarte o repeticiones, con su razonamiento correspondiente:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDjyzPwSE2qg"
      },
      "outputs": [],
      "source": [
        "# Ejemplo exportar archivo de datos (CSV) con registros eliminados por términos de descarte en diccionarios y repeticiones\n",
        "df_eliminados_final.to_csv(f\"{project_name.value}_registros-eliminados.csv\")\n",
        "\n",
        "print(f\"¡{project_name.value}_registros-eliminados.csv descargado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3rKx63aGRWC"
      },
      "source": [
        "## 6. Referencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9oPZQaEekMJ"
      },
      "source": [
        "*   Bird, Steven, Edward Loper & Ewan Klein (2009).\n",
        "Natural Language Processing with Python.  O'Reilly Media Inc.\n",
        "* Kiss, T., & Strunk, J. (2006). Unsupervised Multilingual Sentence Boundary Detection. Computational Linguistics, 32(4), 485-525. https://doi.org/10.1162/coli.2006.32.4.485\n",
        "\n",
        "*Programación asistida ocasionalmente con herramientas de IA Generativa: ChatGPT, Phind, Google Gemini y Perplexity"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Créditos"
      ],
      "metadata": {
        "id": "ASBLPGoSNBqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Realizado por el equipo de Signa_Lab ITESO:**\n",
        "\n",
        "- **Programación de cuadernos de código (Python)**:\n",
        "Javier de la Torre Silva, José Luis Almendarez González y Diego Arredondo Ortiz\n",
        "\n",
        "- **Supervisión del desarrollo tecnológico y documentación:**\n",
        "Diego Arredondo Ortiz\n",
        "\n",
        "- **Equipo de Coordinación Signa_Lab ITESO:**\n",
        "Paloma López Portillo Vázquez, Víctor Hugo Ábrego Molina y Eduardo G. de Quevedo Sánchez\n",
        "\n",
        "Mayo, 2024. Instituto Tecnológico y de Estudios Superiores de Occidente (ITESO)\n",
        "Tlaquepaque, Jalisco, México.\n"
      ],
      "metadata": {
        "id": "fdAyvK0mOkwZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S_ePFxKPJuL"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ReN-KpvrJ-jS",
        "o2-KsJ87KOiZ",
        "O9kQer1lRAg5",
        "l8Fx8oCP7tYv",
        "QVfoXAtilHbz",
        "f86YTwWJlHb0",
        "KL71R1c4O6cp",
        "MS2u9S1B-W2_",
        "PncydP2EPWYU",
        "H3dvOzzoPeVs",
        "m0Hh2sqiP4Ck",
        "Qi6zLeREQCLZ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}