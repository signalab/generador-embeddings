{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HtQx5zDeIb0"
      },
      "source": [
        "# **Signa_Lab ITESO:** Generador de *Embbeddings*\n",
        "\n",
        "## **Cuaderno 01:** Limpieza y depuración de texto para procesar *embeddings* desde cuerpos de texto."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LyCYY7SLPJtv"
      },
      "source": [
        "Cuaderno de código abierto diseñado para importar cualquier cuerpo de texto separado por filas, en formato CSV o Excel, limpiarlo (*stopwords*, URLs, usuarios y hashtags) y depurarlo desde [diccionarios personalizados](https://drive.google.com/file/d/1zK214W0pBRYn9lnY_MDJYhEEc6pI3L6F/view?usp=drive_link) (opcional) para optimizar la posterior generación de incrustaciones de texto (*embeddings*) de cada fila ([ver cuaderno 02](https://github.com/signalab/generador-embeddings/blob/main/cuadernos/02_Signa_Lab_generador_embeddings_Generar_procesar_reducir_clusterizar_embeddings_01.ipynb)), con ayuda de modelos de lenguaje de la librería [sentence-transformers](https://www.sbert.net/), alojados en repositorios de [HuggingFace](https://huggingface.co/sentence-transformers) (en la nube) o descargados localmente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cD-wOGZQ5Ki6"
      },
      "source": [
        "## 1. Importar librerías y archivos de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalar e importar librerías:"
      ],
      "metadata": {
        "id": "ReN-KpvrJ-jS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOuliduEPJtw"
      },
      "source": [
        "**Instalar librerías necesarias**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kidkEvPHPJtw"
      },
      "outputs": [],
      "source": [
        "# Instalar librerías de Python necesarias\n",
        "\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install difflib\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install scipy\n",
        "!pip install numpy\n",
        "!pip install plotly\n",
        "!pip install time\n",
        "!pip install tqdm\n",
        "!pip install operator\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi3kGn_FLC6T"
      },
      "source": [
        "**Importar librerías** necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZ3xsq9LK2Sr"
      },
      "outputs": [],
      "source": [
        "# Importar librerías de Python necesarias\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import sys\n",
        "import re\n",
        "from difflib import SequenceMatcher\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "from scipy.stats import gaussian_kde\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import math\n",
        "import operator\n",
        "from IPython.display import display, clear_output\n",
        "import ipywidgets as widgets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Indicar rutas de archivos de datos a importar y nombre de proyecto:"
      ],
      "metadata": {
        "id": "o2-KsJ87KOiZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfsq2zLpPJtx"
      },
      "source": [
        "**Importar y concatenar archivos de datos** (en CSV o Excel):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para cargar archivos a partir de la extensión en su ruta indicada\n",
        "def load_file(path):\n",
        "    if path.endswith('.csv'):\n",
        "        return pd.read_csv(path)\n",
        "    elif path.endswith('.xlsx'):\n",
        "        return pd.read_excel(path)\n",
        "    else:\n",
        "        raise ValueError(\"Formato no compatible. Por favor carga solo archivos .csv or .xlsx.\")\n",
        "\n",
        "# Inicializar lista para alojar todas las rutas y una variable para el DataFrame final, accesible globalmente\n",
        "file_paths = []\n",
        "dfs = []\n",
        "df = None  # DataFrame global\n",
        "\n",
        "# Definir función para añadir un nuevo campo de texto (input) para añadir una ruta de archivo adicional\n",
        "def add_file_input(b=None):\n",
        "    path_input = widgets.Text(value='', placeholder='Escribe la ruta del archivo', description=f'File {len(file_paths) + 1}:')\n",
        "    file_paths.append(path_input)\n",
        "    update_ui()\n",
        "\n",
        "# Definir función para eliminar el último campo de texto (input) para ruta de archivo\n",
        "def remove_file_input(b=None):\n",
        "    if file_paths:\n",
        "        file_paths.pop()\n",
        "        update_ui()\n",
        "\n",
        "# Definir función para procesar y cargar todos los archivos\n",
        "def process_files(b):\n",
        "    global dfs, df\n",
        "    dfs = []  # Vaciar DataFrames\n",
        "\n",
        "    for path_input in file_paths:\n",
        "        path = path_input.value\n",
        "        try:\n",
        "            temp_df = load_file(path)\n",
        "            temp_df['filename'] = path  # Add a column with the filename\n",
        "            dfs.append(temp_df)\n",
        "            print(f\"Nombre de archivo: {path}\")\n",
        "            print(f\"Filas/Columnas (shape): {temp_df.shape}\")\n",
        "        except ValueError as e:\n",
        "            print(f\"Error al cargar el archivo {path}: {e}\")\n",
        "            return\n",
        "\n",
        "    if dfs:\n",
        "        df = pd.concat(dfs, ignore_index=True)  # Concatenate all DataFrames\n",
        "        print(\"\\n¡Se cargaron todos los archivos!\")\n",
        "        print(f\"Filas/Columnas (shape) de DataFrame creado: {df.shape}\")\n",
        "\n",
        "# Campo de texto (input) para indicar nombre del proyecto (para integrarse en nombres de archivos a exportar)\n",
        "project_name = widgets.Text(value='', placeholder='Escribe el nombre del proyecto (corto y sin espacios)', description='Nombre del proyecto:')\n",
        "\n",
        "# Botones para añadir y eliminar archivos\n",
        "add_button = widgets.Button(description=\"Añadir archivo\")\n",
        "remove_button = widgets.Button(description=\"Eliminar archivo\")\n",
        "load_button = widgets.Button(description=\"Cargar archivos\")\n",
        "\n",
        "add_button.on_click(add_file_input)\n",
        "remove_button.on_click(remove_file_input)\n",
        "load_button.on_click(process_files)\n",
        "\n",
        "# Definir función para actualizar UI\n",
        "def update_ui():\n",
        "    clear_output()\n",
        "    display(project_name)\n",
        "    for path_input in file_paths:\n",
        "        display(path_input)\n",
        "    display(widgets.HBox([add_button, remove_button]))\n",
        "    display(load_button)\n",
        "\n",
        "# Inicializar UI con un campo de texto (input) para ruta de archivo\n",
        "add_file_input()\n"
      ],
      "metadata": {
        "id": "hEtma1lgB6Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previsualizar datos importados:"
      ],
      "metadata": {
        "id": "O9kQer1lRAg5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HptxpF1lHby"
      },
      "source": [
        "**Previsualizar tabla** con todos los registros importados:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsualizar dataframe con CSVs importados\n",
        "display(df)\n",
        "print(f\"Filas/Columnas (shape) en registros importados: {df.shape}\")\n"
      ],
      "metadata": {
        "id": "IM2Rv28GEHOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exportar copia en CSV con registros importados (concatenados):**"
      ],
      "metadata": {
        "id": "9xaQzFYoasLs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nheXTnM7PJt0"
      },
      "outputs": [],
      "source": [
        "# Exportar archivo CSV con tabla de registros importados (y concatenados, en el caso de múltiples archivos)\n",
        "df.to_csv(f\"{project_name.value}_registros-importados.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hPbcqVvlHbz"
      },
      "source": [
        "---\n",
        "\n",
        "## 2. Limpieza de registros importados"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generar identificadores únicos (IDs) por registro:"
      ],
      "metadata": {
        "id": "l8Fx8oCP7tYv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir función para asignar IDs únicos a cada fila en el data frame indicado como parámetro, comenzando desde '1000001'.\n",
        "def assign_unique_ids(df):\n",
        "    # Inicializar contador para IDs\n",
        "    id_counter = 1000001\n",
        "\n",
        "    # Crear copia de data frame original\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Iterar a través de las filas del data frame\n",
        "    for index, _ in enumerate(df_copy.index):\n",
        "        # Dar formato a ID con ceros adicionales e incorporarlo al data frame\n",
        "        formatted_id = str(id_counter).zfill(7)  # Se asegura de que sea un ID de 7 dígitos, agregando ceros cuando sea necesario\n",
        "        df_copy.loc[index, 'id'] = formatted_id\n",
        "\n",
        "        # Incrementar el contador del ID para la siguiente iteración\n",
        "        id_counter += 1\n",
        "\n",
        "    return df_copy"
      ],
      "metadata": {
        "id": "EhuSrvzjr1Df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar función para asignar IDs a cada registro y previsualizar tabla\n",
        "if __name__ == \"__main__\":\n",
        "    # Invocar la función con data frame de trabajo\n",
        "    df_ids = assign_unique_ids(df)\n",
        "\n",
        "# Sobreescribir data frame con nueva tabla con IDs generados\n",
        "df = df_ids\n",
        "df"
      ],
      "metadata": {
        "id": "UyPVvBIWsVzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13e2f_ioPJt8"
      },
      "outputs": [],
      "source": [
        "# Revisar el número de filas y columnas en tabla de registros con IDs generados\n",
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVfoXAtilHbz"
      },
      "source": [
        "### Limpieza de texto sin aporte semántico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYQEQnPllHb0"
      },
      "outputs": [],
      "source": [
        "# Definir función para limpiar usuarios, hashtags y URLs\n",
        "def limpiar_texto(texto):\n",
        "  # Eliminar usuarios (opcional, comentar siguiente línea para omitirlo)\n",
        "  # texto = re.sub(r\"(?<!\\w)@(\\w+)(?!\\w)\", \"\", texto)\n",
        "\n",
        "  # Eliminar hashtags (opcional, comentar siguiente línea para omitirlo)\n",
        "  # texto = re.sub(r\"(?<!\\w)#(\\w+)(?!\\w)\", \"\", texto)\n",
        "\n",
        "  # Eliminar URLs (opcional, comentar siguiente línea para omitirlo)\n",
        "  texto = re.sub(r\"(http|https|ftp)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\", \"\", texto)\n",
        "  texto = texto.lstrip(\". \")\n",
        "\n",
        "  return texto.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thOE2xz6lHb0"
      },
      "source": [
        "**Ejecutar funciones y agregar nueva columna con registros con texto limpio:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_asTAXQzlHb0"
      },
      "outputs": [],
      "source": [
        "# Definir función para agregar nueva columna con texto limpio (clean_text)\n",
        "def agregarCleanTextADf(df, colText):\n",
        "    dfW = df.copy()\n",
        "    dfW[\"clean_text\"] = None\n",
        "\n",
        "    for index, row in dfW.iterrows():\n",
        "        text = str(row[colText])\n",
        "        cleaned_text = limpiar_texto(text)\n",
        "        dfW.at[index, \"clean_text\"] = cleaned_text\n",
        "\n",
        "    return dfW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4cODeeVlHb0"
      },
      "outputs": [],
      "source": [
        "# Especificar nombre de columna con texto para limpiar y procesar embeddings\n",
        "# text_column = \"nombre-columna-texto\"\n",
        "\n",
        "text_column = \"title\"\n",
        "\n",
        "\n",
        "# Ejecutar limpieza de usuarios, hashtags y URLs\n",
        "dfCleanText = agregarCleanTextADf(df, text_column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VgY4nY_lHb0"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla con nueva columna de datos limpios (clean_text)\n",
        "dfCleanText.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f86YTwWJlHb0"
      },
      "source": [
        "### Eliminar palabras vacías (*stop words*):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMi4M_w-lHb0"
      },
      "source": [
        "Descargar dependencias de librería NLTK para **limpieza de palabras vacías (*stop words*):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kz81G4vTlHb0"
      },
      "outputs": [],
      "source": [
        "# Descargar las stop words en español e inglés\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Lista de stopwords en español\n",
        "stopwords_es = nltk.corpus.stopwords.words('spanish')\n",
        "# stopwords_en = nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1BMaiH0lHb1"
      },
      "source": [
        "**Función para eliminar palabras vacías (*stop words*)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8l3-6rvlHb1"
      },
      "outputs": [],
      "source": [
        "# Definir función para eliminar palabras vacías (stopwords) y signos\n",
        "def delete_stopwords(texto):\n",
        "\n",
        "  # Tokenizar el texto\n",
        "  tokens = nltk.word_tokenize(texto)\n",
        "\n",
        "  # Eliminar signos de puntuación\n",
        "  tokens = [token for token in tokens if token.isalnum()]\n",
        "\n",
        "  # Eliminar stop words\n",
        "  tokens = [token for token in tokens if token not in stopwords_es]\n",
        "\n",
        "  # Convertir la lista de tokens a un string\n",
        "  texto_limpio = \" \".join(tokens)\n",
        "\n",
        "  return texto_limpio.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uTm6hRilHb1"
      },
      "source": [
        "Ejecutar funciones y **agregar columna con texto sin *stop words* (sem_text):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWux9PURlHb1"
      },
      "outputs": [],
      "source": [
        "# Definir función para eliminar palabras vacías y agregar nueva columna con el resultado (sem_text)\n",
        "def agregarSemTextADf(df, colText):\n",
        "    dfW = df.copy()\n",
        "    dfW[\"sem_text\"] = None\n",
        "\n",
        "    for index, row in dfW.iterrows():\n",
        "        text = row[colText]\n",
        "        cleaned_text = delete_stopwords(text)\n",
        "        dfW.at[index, \"sem_text\"] = cleaned_text\n",
        "\n",
        "    return dfW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnZelINHlHb1"
      },
      "outputs": [],
      "source": [
        "# Ejecutar función para eliminar palabras vacías y agregar nueva columna con el resultado (sem_text)\n",
        "dfClean = agregarSemTextADf(dfCleanText, \"clean_text\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nZBtGY0lHb1"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de datos con nueva columna de registros con texto sin palabras vacías (sem_text)\n",
        "dfClean.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GlzbI6mPJt-"
      },
      "outputs": [],
      "source": [
        "# Revisar el número de filas y columnas en tabla de registros con texto sin palabras vacías (sem_text)\n",
        "dfClean.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPu5tTLxPJt-"
      },
      "outputs": [],
      "source": [
        "# Exportar archivo CSV con tabla completa de registros importados con IDs y texto sin palabras vacías (sem_text)\n",
        "dfClean.to_csv(f\"{project_name.value}_registros-semtext.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHNNKJdTlHb1"
      },
      "source": [
        "## 3. Depuración de registros desde diccionarios personalizados (con términos de filtrado o descarte)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definir uso de de diccionario de depuración:"
      ],
      "metadata": {
        "id": "KL71R1c4O6cp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_t-7ye11YwN2"
      },
      "source": [
        "A partir de diccionarios personalizados con términos de descarte o filtración, elige depurar o mantener, respectivamente, registros que los contengan.\n",
        "\n",
        "El diccionario debe cargarse en formato CSV y contener, al menos, los siguientes campos:\n",
        "\n",
        "| palabra | tipo | categoría | diccionario |\n",
        "|---------|------|-----------|-------------|\n",
        "|         |      |           |             |\n",
        "|         |      |           |             |\n",
        "|         |      |           |             |\n",
        "\n",
        "Ejemplo:\n",
        "\n",
        "| palabra | tipo   | categoría   | diccionario        |\n",
        "|---------|--------|-------------|--------------------|\n",
        "| idiota  | ofensa | humillación | ofensa-humillación |\n",
        "| zorra   | ofensa | género      | ofensa-género      |\n",
        "|         |        |             |                    |\n",
        "\n",
        "Puedes [encontrar aquí una copia del diccionario](https://drive.google.com/file/d/1zK214W0pBRYn9lnY_MDJYhEEc6pI3L6F/view?usp=sharing) con la estructura requerida, en formato CSV, para descargar, llenar e incorporar a este cuaderno de código.\n",
        "\n",
        "**Nota:** La versión actual de este cuaderno de código permite elegir entre las siguientes opciones sobre la carga de diccionarios:\n",
        "\n",
        "\n",
        "- ***No cargar diccionario:*** Ignorar esta funcionalidad y no usar diccionarios.\n",
        "- ***Diccionario de descarte (negativo):*** Se eliminarán todos los registros que mencionen alguno de esos términos.\n",
        "- ***Diccionario de filtrado (positivo):*** Se mantendrán solo los registros que mencionen alguno de esos términos."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dropdown widget for dictionary usage selection\n",
        "cargar_diccionario = widgets.Dropdown(\n",
        "    options=[\"No cargar diccionario\", \"Diccionario de descarte (negativo)\", \"Diccionario de filtrado (positivo)\"],\n",
        "    value=\"No cargar diccionario\",\n",
        "    description=\"Elegir uso:\"\n",
        ")\n",
        "\n",
        "# Create the text input widget for file path\n",
        "rutaDicc = widgets.Text(\n",
        "    value=\"\",\n",
        "    placeholder=\"Escribir ruta a archivo CSV\",\n",
        "    description=\"Ruta:\"\n",
        ")\n",
        "\n",
        "# Create the \"Aceptar\" button\n",
        "accept_button = widgets.Button(description=\"Aceptar\")\n",
        "\n",
        "# Create an output widget to display results or errors\n",
        "output = widgets.Output()\n",
        "\n",
        "# Function to load the dictionary based on user input\n",
        "def load_dictionary(b):\n",
        "    with output:\n",
        "        clear_output()\n",
        "        if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "            try:\n",
        "                global dfDiccionario\n",
        "                dfDiccionario = pd.read_csv(rutaDicc.value)\n",
        "                print(f\"Archivo de diccionario '{rutaDicc.value}' cargado exitosamente.\")\n",
        "                display(dfDiccionario.head())  # Show the first few rows of the loaded dictionary\n",
        "            except FileNotFoundError:\n",
        "                print(f\"El archivo de diccionario '{rutaDicc.value}' no fue encontrado. Por favor, verifique la ruta.\")\n",
        "        else:\n",
        "            dfDiccionario = None\n",
        "            print(\"No se cargó ningún diccionario.\")\n",
        "\n",
        "# Link the button to the load function\n",
        "accept_button.on_click(load_dictionary)\n",
        "\n",
        "# Display the widgets\n",
        "display(cargar_diccionario, rutaDicc, accept_button, output)\n"
      ],
      "metadata": {
        "id": "AuzxsTVCg-uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verificar uso de diccionario elegido\n",
        "print(f\"Uso de diccionario elegido: {cargar_diccionario.value}\")"
      ],
      "metadata": {
        "id": "24zZKqwCSgo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Aplicar filtrado por diccionario y previsualizar resultados (opcional, solo si se cargó algún diccionario):"
      ],
      "metadata": {
        "id": "MS2u9S1B-W2_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hj8iG14lHb2"
      },
      "source": [
        "**Previsualizar diccionario de descarte importado:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_D1HZSAlHb2"
      },
      "outputs": [],
      "source": [
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  # Previsualizar tabla de diccionario cargado (en caso de haber elegido utilizar diccionarios)\n",
        "  try:\n",
        "      display(dfDiccionario.head())  # Previsualizar primeros registros de diccionario\n",
        "      print(f\"\\nFilas y columnas en diccionario cargado:\")  # Verificar número de filas y columnas en diccionario\n",
        "      print(f\"Shape: {dfDiccionario.shape}\")  # Verificar número de filas y columnas en diccionario\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Filtrar registros a partir de diccionario cargado y modalidad de depuración (positiva o negativa):**"
      ],
      "metadata": {
        "id": "JVUqxmIe6LHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filtrar_registros(df_registros, df_terminos, colTexto, cargar_diccionario):\n",
        "    # Definir los términos del diccionario\n",
        "    terminos = df_terminos[\"palabra\"].tolist()\n",
        "    # Compilar expresiones regulares una sola vez\n",
        "    expresiones_regex = [\n",
        "        re.compile(r\"(?<!\\S)?(?:\\s|[.,;:?!¡¿]){}(?:\\s|[.,;:?!¡¿])?(?!\\S)\".format(re.escape(termino)), re.IGNORECASE)\n",
        "        for termino in terminos\n",
        "    ]\n",
        "\n",
        "    # Copiar el dataframe de registros y agregar columnas auxiliares\n",
        "    df_registros_filtrados = df_registros.copy()\n",
        "    df_registros_filtrados[\"contiene_termino\"] = False\n",
        "    df_registros_filtrados[\"razon_eliminacion\"] = \"\"\n",
        "\n",
        "    # Filtrar registros\n",
        "    for i in range(df_registros_filtrados.shape[0]):\n",
        "        texto = str(df_registros_filtrados.loc[i, colTexto]).lower().replace(\"á\", \"a\").replace(\"é\", \"e\").replace(\"í\", \"i\").replace(\"ó\", \"o\").replace(\"ú\", \"u\")\n",
        "\n",
        "        # Buscar coincidencias con expresiones regulares\n",
        "        for expresion, razon in zip(expresiones_regex, df_terminos[\"categoría\"]):\n",
        "            coincidencias = expresion.findall(f\" {texto} \")\n",
        "            if coincidencias:\n",
        "                df_registros_filtrados.loc[i, \"contiene_termino\"] = True\n",
        "                df_registros_filtrados.loc[i, \"razon_eliminacion\"] = f\"Presencia de términos relacionados a {razon}\"\n",
        "                break\n",
        "            else:\n",
        "                df_registros_filtrados.loc[i, \"contiene_termino\"] = False\n",
        "                df_registros_filtrados.loc[i, \"razon_eliminacion\"] = f\"Ausencia de términos relacionados a {razon}\"\n",
        "\n",
        "    # Revisar uso elegido del diccionario, para descarte (negativo) o filtrado (positivo)\n",
        "    if cargar_diccionario == \"Diccionario de descarte (negativo)\":\n",
        "        # Mantener registros que no contengan términos del diccionario (negativo)\n",
        "        df_registros_filtrados_final = df_registros_filtrados[~df_registros_filtrados[\"contiene_termino\"]]\n",
        "        df_registros_eliminados = df_registros_filtrados[df_registros_filtrados[\"contiene_termino\"]]\n",
        "    elif cargar_diccionario == \"Diccionario de filtrado (positivo)\":\n",
        "        # Mantener registros que sí contengan al menos un término del diccionario (positivo)\n",
        "        df_registros_filtrados_final = df_registros_filtrados[df_registros_filtrados[\"contiene_termino\"]]\n",
        "        df_registros_eliminados = df_registros_filtrados[~df_registros_filtrados[\"contiene_termino\"]]\n",
        "\n",
        "    # Eliminar columnas auxiliares para tabla con registros depurados\n",
        "    df_registros_filtrados_final = df_registros_filtrados_final.drop(columns=[\"contiene_termino\", \"razon_eliminacion\"])\n",
        "\n",
        "    df_registros_filtrados_final = df_registros_filtrados_final.reset_index(drop=True)\n",
        "    df_registros_eliminados = df_registros_eliminados.reset_index(drop=True)\n",
        "\n",
        "    return df_registros_filtrados_final, df_registros_eliminados\n"
      ],
      "metadata": {
        "id": "R6lyxcaMHx6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dgm6-sYlHb2"
      },
      "outputs": [],
      "source": [
        "# Ejecutar depuración de registros por diccionario cargado\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  # try:\n",
        "  df_depurados_dicc, df_eliminados_dicc = filtrar_registros(dfClean, dfDiccionario, text_column, cargar_diccionario.value)\n",
        "  # except NameError:\n",
        "      # print(\"Otro error...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Previsualizar registros depurados y eliminados (opcional):**"
      ],
      "metadata": {
        "id": "U4dnWce36Uek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsualizar tabla de registros depurados (conservados) y verificar número de filas y columna\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  try:\n",
        "    display(df_depurados_dicc.head())\n",
        "    print(f\"\\nFilas/Columnas (shape) en registros conservados por diccionario: {df_depurados_dicc.shape}\")\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")"
      ],
      "metadata": {
        "id": "RGDYqSyOlZVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Previsualizar tabla de registros eliminados y verificar su número de filas y columnas\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  try:\n",
        "    display(df_eliminados_dicc.head())\n",
        "    print(f\"\\nFilas/Columnas (shape) en registros eliminados por diccionario: {df_eliminados_dicc.shape}\")\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")"
      ],
      "metadata": {
        "id": "8qHoZTZV6bZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzbnrrTKPJuA"
      },
      "outputs": [],
      "source": [
        "# Verificar eliminación de casos específicos en registros eliminados o conservados (opcional)\n",
        "\n",
        "# Indicar palabra a buscar\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  verificar_termino = \"vandalismo\"\n",
        "\n",
        "  _count = 0\n",
        "  # Buscar palabra en registros conservados\n",
        "  for i in df_depurados_dicc['sem_text']:\n",
        "\n",
        "  # Buscar palabra en registros eliminados\n",
        "  # for i in df_eliminados_dicc['sem_text']:\n",
        "\n",
        "      words = i.split()\n",
        "      if verificar_termino in words:\n",
        "          _count += 1\n",
        "  print(_count)\n",
        "else:\n",
        "  print(\"No se cargó ningún diccionario...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Revisar, contar y eliminar registros repetidos"
      ],
      "metadata": {
        "id": "94TJeKlg_GlP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Definir y aplicar función para eliminar redacciones repetidas:"
      ],
      "metadata": {
        "id": "PncydP2EPWYU"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2nOcuD2lpAk"
      },
      "source": [
        "**Eliminar registros repetidos:**\n",
        "\n",
        "Eliminar aquellos registros que contengan una similitud en su redacción mayor a un umbral establecido (por default asignado al 100%, con valor de 1), para así buscar eliminar registros con una repetición exacta.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5YSdAyiPJuA"
      },
      "outputs": [],
      "source": [
        "# Definir función para calcular la similitud entre dos listas de palabras\n",
        "def Similarity_Score(list1, list2):\n",
        "    # Inicializar contadores para coincidencias y longitud total\n",
        "    matches = 0\n",
        "    total_length = 0\n",
        "\n",
        "    # Iterar sobre las listas hasta el tamaño de la lista más corta\n",
        "    for i in range(min(len(list1), len(list2))):\n",
        "        # Si las palabras en las mismas posiciones coinciden, incrementar el contador de coincidencias\n",
        "        if list1[i] == list2[i]:\n",
        "            matches += 1\n",
        "        # Incrementar el contador de longitud total\n",
        "        total_length += 1\n",
        "\n",
        "    # Para las posiciones adicionales en la lista más larga, incrementar el contador de longitud total\n",
        "    for i in range(min(len(list1), len(list2)), max(len(list1), len(list2))):\n",
        "        total_length += 1\n",
        "\n",
        "    # Calcular el ratio de coincidencias como la proporción de coincidencias sobre la longitud total\n",
        "    ratio = matches / total_length\n",
        "\n",
        "    return ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vS96x0RXPJuA"
      },
      "source": [
        "**Definir función para identificar registros repetidos:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jx-sbH8lrqY"
      },
      "outputs": [],
      "source": [
        "def remove_duplicates_with_threshold(df, column, threshold):\n",
        "    global similarity_score\n",
        "    print(\"Se actualizó\")\n",
        "    indices_to_remove = set()\n",
        "    sentence_frequency = defaultdict(int) # Diccionario para almacenar la frecuencia de registros similares\n",
        "    discarded_info = defaultdict(list) # Diccionario para almacenar información de registros descartados\n",
        "\n",
        "    # Crear índice invertido para las palabras en los registros\n",
        "    inverted_index = defaultdict(set)\n",
        "    for i, sentence in enumerate(df[column]):\n",
        "        words = set(sentence.split())\n",
        "        for word in words:\n",
        "            inverted_index[word].add(i)\n",
        "\n",
        "    print(f\"{len(df[column])} registros en total\")\n",
        "    for i, sentence in enumerate(df[column]):\n",
        "        if i not in indices_to_remove:\n",
        "            similar_sentences_count = 1 # Contador de registros similares para la fila actual\n",
        "            words = set(sentence.split())\n",
        "            relevant_indices = set()\n",
        "            for word in words:\n",
        "                relevant_indices |= inverted_index[word]\n",
        "\n",
        "            for j in relevant_indices:\n",
        "                if j != i and j not in indices_to_remove:\n",
        "                    registroSinAcentos = sentence.replace('á', 'a').replace('é','e').replace('í','i').replace('ó','o').replace('ú','u')\n",
        "                    registroSinAcentosEnLista = registroSinAcentos.split(\" \")\n",
        "                    registroAComparar = df[column][j]\n",
        "                    registroACompararSinAcentos = registroAComparar.replace('á', 'a').replace('é','e').replace('í','i').replace('ó','o').replace('ú','u')\n",
        "                    registroACompararEnLista = registroACompararSinAcentos.split(\" \")\n",
        "\n",
        "                    score = Similarity_Score(list(registroSinAcentosEnLista), list(registroACompararEnLista))\n",
        "                    if score >= threshold:\n",
        "                        indices_to_remove.add(j)\n",
        "                        similar_sentences_count += 1\n",
        "                        # Almacenar información de la registro descartado\n",
        "                        discarded_info[j].append({'original_index': df['id'][i], 'similarity_score': score})\n",
        "            sentence_frequency[i] = similar_sentences_count # Almacenar la frecuencia de registros similares para la fila actual\n",
        "\n",
        "        if i % 1000 == 0:\n",
        "            print(f\"Van {i} registros revisados...\")\n",
        "\n",
        "    # Eliminar los registros duplicados después de completar el bucle\n",
        "    filtered_df = df.drop(indices_to_remove).reset_index(drop=True)\n",
        "\n",
        "    # Crear DataFrame con registros duplicados\n",
        "    df_removed_duplicates = df.iloc[list(indices_to_remove)]\n",
        "\n",
        "    # Agregar información de registros descartados al DataFrame de registros descartados\n",
        "    id_match = []\n",
        "    similarity_score = []\n",
        "\n",
        "    # Iterar sobre índice de DataFrame\n",
        "    for index in df_removed_duplicates.index:\n",
        "        # Revisar si el índice se encuentra en discarded_info\n",
        "        if index in discarded_info:\n",
        "            # Por cada índice, toma el primer elemento de 'original_index' y 'similarity_score'\n",
        "            id_match.append(discarded_info[index][0]['original_index'])\n",
        "            similarity_score.append(discarded_info[index][0]['similarity_score'])\n",
        "        else:\n",
        "            # Si el índice no está en discarded_info, agregar el valor por default None\n",
        "            id_match.append(None)\n",
        "            similarity_score.append(None)\n",
        "\n",
        "    df_removed_duplicates['id_match'] = id_match\n",
        "    df_removed_duplicates['similarity_score'] = similarity_score\n",
        "\n",
        "    filtered_df['sentence_frequency_count'] = filtered_df['id'].apply(\n",
        "        lambda x: len(df_removed_duplicates[df_removed_duplicates['id_match'] == x]) + 1)\n",
        "\n",
        "    return filtered_df, df_removed_duplicates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoKv1HZbPJuA"
      },
      "source": [
        "Ejecutar función para **eliminar registros duplicados**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roueuL7IPJuB"
      },
      "outputs": [],
      "source": [
        "# Establecer umbral de similitud (porcentaje 0 a 100%, indicando un valor de 0 a 1)\n",
        "threshold = 1 # Con 1, se eliminan registros que sean 100% similares en su redacción a alguno ya registrado\n",
        "# Ejecutar elminación de registros repetidos\n",
        "\n",
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  df_depurados_final, df_removed_duplicates = remove_duplicates_with_threshold(df_depurados_dicc, 'sem_text', threshold)\n",
        "else:\n",
        "  df_depurados_final, df_removed_duplicates = remove_duplicates_with_threshold(dfClean, 'sem_text', threshold)\n",
        "  print(\"No se cargó ningún diccionario...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previsualizar resultados de conteo y depuración de registros repetidos:"
      ],
      "metadata": {
        "id": "H3dvOzzoPeVs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzqdvJWJNxtu"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de población de registros depurados con la frecuencia de aparición de cada registro respecto a otros registros\n",
        "df_depurados_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4sOXpOtlvXC"
      },
      "outputs": [],
      "source": [
        "# Definir función para agregar la razón de eliminación por repetidos\n",
        "def agregar_razon_eliminacion(df_removed, razon):\n",
        "    df_removed['razon_eliminacion'] = razon\n",
        "    return df_removed\n",
        "\n",
        "df_removed_duplicates = agregar_razon_eliminacion(df_removed_duplicates, 'Redacción repetida respecto a otro registro')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDpFQRBGNxuA"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de registros eliminados por repetición con razón de eliminación\n",
        "df_removed_duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y5PWCG9PJuB"
      },
      "outputs": [],
      "source": [
        "# Revisar el número de filas y columnas en tabla de registros eliminados por repetición\n",
        "df_removed_duplicates.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-le8Mw8OH5v"
      },
      "source": [
        "## 5. Revisar y exportar datos con registros depurados y eliminados"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consolidar y previsualizar tablas finales de registros depurados y eliminados:"
      ],
      "metadata": {
        "id": "m0Hh2sqiP4Ck"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtD-LevA7VCm"
      },
      "source": [
        "**Consolidar tabla de registros eliminados, concatenando tablas de eliminados por diccionario (en caso de haberse cargado) y por repeticiones:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WmXzxCEE7dq-"
      },
      "outputs": [],
      "source": [
        "if cargar_diccionario.value != \"No cargar diccionario\":\n",
        "  try:\n",
        "    # Ejecutar concatenación de filas eliminadas por términos en diccionario y repeticiones\n",
        "    df_eliminados_final = pd.concat([df_eliminados_dicc, df_removed_duplicates], axis=0)\n",
        "  except NameError:\n",
        "      print(\"No se cargó ningún diccionario...\")\n",
        "else:\n",
        "  df_eliminados_final = df_removed_duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJMKBy_ANxuA"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de registros totales eliminados (por términos en diccionario y repeticiones), con razón de eliminación\n",
        "display(df_eliminados_final)\n",
        "print(f\"Filas/Columnas (shape) en resgistros totales eliminados: {df_eliminados_final.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xf81B8Q2NxuB"
      },
      "outputs": [],
      "source": [
        "# Previsualizar tabla de registros totales depurados (conservados)\n",
        "display(df_depurados_final)\n",
        "print(f\"Filas/Columnas (shape) en registros totales conservados: {df_depurados_final.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exportar archivos CSV con tablas finales de registros depurados y elminados:"
      ],
      "metadata": {
        "id": "Qi6zLeREQCLZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuLWEw8llHb2"
      },
      "source": [
        "**Exportar archivo de datos (en formato CSV) de población de registros depurados a utilizar:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkCxiCz-lHb2"
      },
      "outputs": [],
      "source": [
        "# Ejemplo exportar archivo de datos (CSV) con población de registros depurados\n",
        "df_depurados_final.to_csv(f\"{project_name.value}_poblacion-registros-depurados.csv\")\n",
        "\n",
        "print(f\"¡{project_name.value}_poblacion-registros-depurados.csv descargado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7O5EaHqz8rXG"
      },
      "source": [
        "**Exportar archivo de datos (en formato CSV) de registros eliminados por términos de descarte o repeticiones, con su razonamiento correspondiente:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDjyzPwSE2qg"
      },
      "outputs": [],
      "source": [
        "# Ejemplo exportar archivo de datos (CSV) con registros eliminados por términos de descarte en diccionarios y repeticiones\n",
        "df_eliminados_final.to_csv(f\"{project_name.value}_registros-eliminados.csv\")\n",
        "\n",
        "print(f\"¡{project_name.value}_registros-eliminados.csv descargado!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3rKx63aGRWC"
      },
      "source": [
        "## 6. Referencias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9oPZQaEekMJ"
      },
      "source": [
        "*   Bird, Steven, Edward Loper & Ewan Klein (2009).\n",
        "Natural Language Processing with Python.  O'Reilly Media Inc.\n",
        "* Kiss, T., & Strunk, J. (2006). Unsupervised Multilingual Sentence Boundary Detection. Computational Linguistics, 32(4), 485-525. https://doi.org/10.1162/coli.2006.32.4.485"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Créditos"
      ],
      "metadata": {
        "id": "ASBLPGoSNBqe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Realizado por el equipo de Signa_Lab ITESO:**\n",
        "\n",
        "- **Programación de cuadernos de código (Python)**:\n",
        "Javier de la Torre Silva, José Luis Almendarez González y Diego Arredondo Ortiz\n",
        "\n",
        "- **Supervisión del desarrollo tecnológico y documentación:**\n",
        "Diego Arredondo Ortiz\n",
        "\n",
        "- **Equipo de Coordinación Signa_Lab ITESO:**\n",
        "Paloma López Portillo Vázquez, Víctor Hugo Ábrego Molina y Eduardo G. de Quevedo Sánchez\n",
        "\n",
        "Mayo, 2024. Instituto Tecnológico y de Estudios Superiores de Occidente (ITESO)\n",
        "Tlaquepaque, Jalisco, México.\n"
      ],
      "metadata": {
        "id": "fdAyvK0mOkwZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S_ePFxKPJuL"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ReN-KpvrJ-jS",
        "o2-KsJ87KOiZ",
        "O9kQer1lRAg5",
        "6hPbcqVvlHbz",
        "l8Fx8oCP7tYv",
        "QVfoXAtilHbz",
        "f86YTwWJlHb0",
        "KL71R1c4O6cp",
        "MS2u9S1B-W2_",
        "94TJeKlg_GlP",
        "m0Hh2sqiP4Ck",
        "Qi6zLeREQCLZ",
        "a3rKx63aGRWC",
        "ASBLPGoSNBqe"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}